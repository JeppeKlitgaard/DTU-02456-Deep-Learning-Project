{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23325c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import rarfile\n",
    "import webdataset as wds\n",
    "from tqdm.auto import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed4d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUANG_DATASET_REPO_ID = \"hoang-quoc-trung/fusion-image-to-latex-datasets\"\n",
    "\n",
    "THIS_FOLDER = Path().resolve()\n",
    "DATA_FOLDER = THIS_FOLDER / \"data\"\n",
    "DATA_FOLDER.mkdir(exist_ok=True, parents=True)\n",
    "TEST_FOLDER = DATA_FOLDER / \"test\"\n",
    "TEST_FOLDER.mkdir(exist_ok=True, parents=True)\n",
    "TRAIN_FOLDER = DATA_FOLDER / \"train\"\n",
    "TRAIN_FOLDER.mkdir(exist_ok=True, parents=True)\n",
    "VALIDATION_FOLDER = DATA_FOLDER / \"validation\"\n",
    "VALIDATION_FOLDER.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c52c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure rarfile\n",
    "rarfile.UNRAR_TOOL = str(THIS_FOLDER / \"UnRAR.exe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a06f841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/jeppe/.cache/huggingface/hub/datasets--hoang-quoc-trung--fusion-image-to-latex-datasets/blobs/afe2298da7eb1bc4410818ca4386331f8c354978f8e4b6fb62a850c0b8e28ed2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download images\n",
    "images_rar_path = Path(hf_hub_download(\n",
    "    repo_id=HUANG_DATASET_REPO_ID,\n",
    "    filename=\"root.rar\",\n",
    "    repo_type=\"dataset\"\n",
    ")).resolve()\n",
    "\n",
    "assert images_rar_path.exists()\n",
    "images_rar_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad016c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "df = pd.read_parquet(DATA_FOLDER / \"metadata.parquet\")\n",
    "df = df.set_index(\"image_filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19551941",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = rarfile.RarFile(images_rar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4692fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Dump filenames\n",
    "filenames = rf.namelist()\n",
    "with open(DATA_FOLDER / \"filenames.txt\", \"w\") as f:\n",
    "    for filename in filenames:\n",
    "        f.write(f\"{filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728802b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing data/test/shard_000000.tar 0 0.0 GB 0\n",
      "# writing data/train/shard_000000.tar 0 0.0 GB 0\n",
      "# writing data/validation/shard_000000.tar 0 0.0 GB 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e050605da0404de2b811f1d056c7d2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing images:   0%|          | 0/3669584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'latex'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     39\u001b[39m metadata_entry = df.loc[filename]\n\u001b[32m     41\u001b[39m metadata = {\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage_type\u001b[39m\u001b[33m\"\u001b[39m: metadata_entry[\u001b[33m\"\u001b[39m\u001b[33mimage_type\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     43\u001b[39m }\n\u001b[32m     45\u001b[39m sample = {\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m__key__\u001b[39m\u001b[33m\"\u001b[39m: key,\n\u001b[32m     47\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimage.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_extension\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: image_data,\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlatex.txt\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlatex\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtypst.txt\u001b[39m\u001b[33m\"\u001b[39m: metadata[\u001b[33m\"\u001b[39m\u001b[33mtypst\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata.json\u001b[39m\u001b[33m\"\u001b[39m: json.dumps(metadata).encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     51\u001b[39m }\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadata_entry[\u001b[33m\"\u001b[39m\u001b[33msplit\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     54\u001b[39m     sink_train.write(sample)\n",
      "\u001b[31mKeyError\u001b[39m: 'latex'"
     ]
    }
   ],
   "source": [
    "# Since random access is horrible in RAR files, we let the RAR file dictate the order of the images\n",
    "shard_writer_args = {\n",
    "    \"maxcount\": 10_000,\n",
    "    \"maxsize\": 1e9,\n",
    "}\n",
    "\n",
    "skip_count = 0\n",
    "\n",
    "with (\n",
    "    # rarfile.RarFile(images_rar_path) as rf,\n",
    "    wds.ShardWriter(\"data/test/shard_%06d.tar\", **shard_writer_args) as sink_test,\n",
    "    wds.ShardWriter(\"data/train/shard_%06d.tar\", **shard_writer_args) as sink_train,\n",
    "    wds.ShardWriter(\"data/validation/shard_%06d.tar\", **shard_writer_args) as sink_val,\n",
    "\n",
    "):\n",
    "    infolist = rf.infolist()\n",
    "\n",
    "    for info in tqdm(infolist, desc=\"Processing images\"):\n",
    "        filename = info.filename\n",
    "\n",
    "        # Fix: corrupted filenames from original dataset\n",
    "        filename = filename.removeprefix(\"images/\")\n",
    "\n",
    "        if filename != \"0000b55567e8c74_basic.png\":\n",
    "            filename = filename.removeprefix(\"0000b55567e8c74_basic.png\")\n",
    "\n",
    "        if filename == \"/\":\n",
    "            continue\n",
    "\n",
    "        key = Path(filename).stem\n",
    "        image_extension = Path(filename).suffix.lstrip(\".\").lower()\n",
    "\n",
    "        if filename not in df.index:\n",
    "            skip_count += 1\n",
    "            print(f\"Skipping {filename} as it is not in the metadata (skip: {skip_count})\")\n",
    "            continue\n",
    "\n",
    "        image_data = rf.read(info)\n",
    "        metadata_entry = df.loc[filename]\n",
    "\n",
    "        metadata = {\n",
    "            \"image_type\": metadata_entry[\"image_type\"],\n",
    "        }\n",
    "\n",
    "        sample = {\n",
    "            \"__key__\": key,\n",
    "            f\"image.{image_extension}\": image_data,\n",
    "            \"latex.txt\": metadata_entry[\"latex\"],\n",
    "            \"typst.txt\": metadata_entry[\"typst\"],\n",
    "            \"metadata.json\": json.dumps(metadata).encode(\"utf-8\"),\n",
    "        }\n",
    "\n",
    "        if metadata_entry[\"split\"] == \"train\":\n",
    "            sink_train.write(sample)\n",
    "        elif metadata_entry[\"split\"] == \"validation\":\n",
    "            sink_val.write(sample)\n",
    "        elif metadata_entry[\"split\"] == \"test\":\n",
    "            sink_test.write(sample)\n",
    "        else:\n",
    "            print(f\"Unknown split for {filename}: {metadata_entry['split']}\")\n",
    "            continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
