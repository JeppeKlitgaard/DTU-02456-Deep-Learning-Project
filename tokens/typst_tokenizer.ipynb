{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import tokenizers\n",
    "import json\n",
    "import pandas as pd\n",
    "from typstscribe.const import TOKENIZER_MODELS_DIR, TOKENS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TYPST_SYNTAX_DIR = TOKENS_DIR / \"typst_syntax\"\n",
    "\n",
    "VOCAB_SIZE = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b27b6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get codex sym tokens\n",
    "CODEX_SYM_TXT_URL = \"https://raw.githubusercontent.com/typst/codex/21b1cd06f61bc90bae32780297f82282c759ccc9/src/modules/sym.txt\"\n",
    "response = requests.get(CODEX_SYM_TXT_URL)\n",
    "\n",
    "assert response.status_code == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca2c79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_symbol_file(text):\n",
    "    \"\"\"\n",
    "    Parses a symbol definition file and yields a list of dot-notation identifiers.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw content of the symbol file.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of identifier strings (e.g., 'space.nobreak').\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Strip comments (everything after //)\n",
    "        line = line.split('//')[0].strip()\n",
    "        if line:\n",
    "            # Add tokens from this line\n",
    "            cleaned_tokens.extend(line.split())\n",
    "\n",
    "    results = []\n",
    "    current_root = None\n",
    "\n",
    "    # Regex to identify a valid Key (identifier)\n",
    "    # Must contain letters or dots, and must have at least one letter.\n",
    "    # This avoids matching \"...\" or purely numeric/symbolic tokens.\n",
    "    key_pattern = re.compile(r'^(?=.*[a-zA-Z])[\\.a-zA-Z]+$')\n",
    "\n",
    "    i = 0\n",
    "    while i < len(cleaned_tokens):\n",
    "        token = cleaned_tokens[i]\n",
    "\n",
    "        # Check if the token is a Key (Identifier)\n",
    "        if key_pattern.match(token):\n",
    "\n",
    "            if token.startswith('.'):\n",
    "                # CASE 1: Child Symbol (e.g., .nobreak)\n",
    "                # It appends to the last known root.\n",
    "                if current_root:\n",
    "                    full_name = f\"{current_root}{token}\"\n",
    "                    results.append(full_name)\n",
    "                # Children in this format always have values, so we implicitly skip the value next loop\n",
    "\n",
    "            else:\n",
    "                # CASE 2: New Root (e.g., space, paren)\n",
    "                current_root = token\n",
    "\n",
    "                # We need to peek ahead to see if this Root is a Symbol or just a Namespace.\n",
    "                # If the NEXT token is NOT a Key, it must be a Value.\n",
    "                # If it is a Value, then this Root is a valid symbol itself (e.g., space \\u{20}).\n",
    "                # If the next token IS a Key (or end of file), this Root is just a container (e.g., paren).\n",
    "\n",
    "                is_symbol = False\n",
    "                if i + 1 < len(cleaned_tokens):\n",
    "                    next_token = cleaned_tokens[i+1]\n",
    "                    # If next token is NOT a key, it's a value\n",
    "                    if not key_pattern.match(next_token):\n",
    "                        is_symbol = True\n",
    "\n",
    "                if is_symbol:\n",
    "                    results.append(current_root)\n",
    "\n",
    "        else:\n",
    "            # Token is a Value (e.g., \\u{2060}, (, ⟮, etc.) or garbage.\n",
    "            # We ignore values as we only want the identifiers.\n",
    "            pass\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return results\n",
    "\n",
    "codex_sym_symbols = parse_symbol_file(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3cfe9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now parse manual symbols\n",
    "def parse_manual_file(text):\n",
    "    \"\"\"\n",
    "    Parses a manual symbol definition file and yields a list of dot-notation identifiers.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw content of the manual symbol file.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of identifier strings (e.g., 'math.styles.upright').\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    lines = [l for l in lines if not l.startswith(\"//\")]  # Remove lines with comments\n",
    "    lines = [l for l in lines if l.strip() != \"\"]  # Remove empty lines\n",
    "\n",
    "    unique_lines = set(lines)\n",
    "    return list(unique_lines)\n",
    "\n",
    "manual_symbols = []\n",
    "for filename in TYPST_SYNTAX_DIR.glob(\"manual/*.txt\"):\n",
    "    with open(filename) as f:\n",
    "        file_content = f.read()\n",
    "        manual_symbols.extend(parse_manual_file(file_content))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf1996cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2628\n"
     ]
    }
   ],
   "source": [
    "raw_typst_tokens = list(set(codex_sym_symbols + manual_symbols))\n",
    "typst_tokens = []\n",
    "\n",
    "# Add versions with space prepended for efficiency\n",
    "UNICODE_SPACE_CHAR = \"\\u0120\"  # Ġ\n",
    "for token in raw_typst_tokens:\n",
    "    typst_tokens.append(token)\n",
    "    typst_tokens.append(UNICODE_SPACE_CHAR + token)\n",
    "\n",
    "print(len(typst_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5cd0e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset for tokenizer\n",
    "ds = load_dataset(\n",
    "    \"JeppeKlitgaard/typst-image-dataset\",\n",
    "    data_files=\"metadata.parquet\",\n",
    ")[\"train\"][\"typst\"][:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a3766f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These come from RoBERTa\n",
    "ARCHITECTURE_TOKENS = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"mask_token\": \"<mask>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5cf617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = list(ARCHITECTURE_TOKENS.values()) + typst_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "592fff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ADD_PREFIX_SPACE = False\n",
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE(unk_token=ARCHITECTURE_TOKENS[\"unk_token\"]))\n",
    "\n",
    "# Pretokenization\n",
    "# ByteLevel encoding to ensure all characters are representable\n",
    "# Split digits into isolated tokens (e.g., \"2024\" -> \"2\", \"0\", \"2\", \"4\")\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Sequence([\n",
    "    tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=ADD_PREFIX_SPACE, use_regex=False),  # Add ByteLevel to prevent <unk> tokens\n",
    "    tokenizers.pre_tokenizers.Digits(individual_digits=True),  # Split digits\n",
    "])\n",
    "# tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "# Train tokeniser\n",
    "trainer = tokenizers.trainers.BpeTrainer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    special_tokens=all_tokens,\n",
    "    initial_alphabet=tokenizers.pre_tokenizers.ByteLevel.alphabet(),\n",
    "    show_progress=True,\n",
    ")\n",
    "BATCH_SIZE = 10_000\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(ds), BATCH_SIZE):\n",
    "        yield ds[i : i + BATCH_SIZE]\n",
    "\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(ds))\n",
    "\n",
    "# Actually this breaks with current Typst math syntax (f_a(x) ≠ f_a (x))\n",
    "# bracket_like = [\"(\", \"[\", \"{\", \")\", \"]\", \"}\"]\n",
    "# for bracket in bracket_like:\n",
    "#     tokenizer.add_tokens([\n",
    "#         tokenizers.AddedToken(\n",
    "#             content=bracket,\n",
    "#             single_word=False,\n",
    "#             special=False,\n",
    "#             # lstrip=True,\n",
    "#             # rstrip=True,\n",
    "#         )\n",
    "#     ])\n",
    "\n",
    "# Decoder\n",
    "tokenizer.decoder = tokenizers.decoders.ByteLevel()\n",
    "\n",
    "eos_id = tokenizer.token_to_id(ARCHITECTURE_TOKENS[\"eos_token\"])\n",
    "bos_id = tokenizer.token_to_id(ARCHITECTURE_TOKENS[\"bos_token\"])\n",
    "\n",
    "tokenizer.post_processor = tokenizers.processors.RobertaProcessing(\n",
    "    (ARCHITECTURE_TOKENS[\"eos_token\"], eos_id),\n",
    "    (ARCHITECTURE_TOKENS[\"bos_token\"], bos_id),\n",
    "    trim_offsets=True,\n",
    "    add_prefix_space=ADD_PREFIX_SPACE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ee0cd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalizer:\", tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85a5410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer to JSON string\n",
    "tokenizer_json = tokenizer.to_str()\n",
    "config = json.loads(tokenizer_json)\n",
    "\n",
    "# Convert to set for faster lookup\n",
    "typst_set = set(typst_tokens)\n",
    "\n",
    "# Iterate through the existing list\n",
    "for item in config.get(\"added_tokens\", []):\n",
    "    if item[\"content\"] in typst_set:\n",
    "        # Unmark as special\n",
    "        item[\"single_word\"] = True\n",
    "        item[\"special\"] = False\n",
    "\n",
    "# Reload tokenizer from modified config\n",
    "tokenizer = tokenizers.Tokenizer.from_str(json.dumps(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03937f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final tokenizer\n",
    "TOKENIZER_DIR = TYPST_SYNTAX_DIR / \"tokenizer\"\n",
    "TOKENIZER_DIR.mkdir(exist_ok=True)\n",
    "tokenizer.save(str(TOKENIZER_DIR / \"typst_tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af2f4124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Lambda _ ( W ) ^ ( ( 0 ) ) ( zeta ; r ) = 1 , wide Lambda _ ( W ) ^ ( ( 1 ) ) ( zeta ; r ) = Lambda ( zeta ; r ) = ( Theta _ ( q ^ ( 4 ) ) ( r zeta ^ ( 2 ) ) Theta _ ( q ^ ( 4 ) ) ( q ^ ( 2 ) r zeta ^ ( - 2 ) ) ) / ( Theta _ ( q ^ ( 4 ) ) ( r zeta ^ ( - 2 ) ) Theta _ ( q ^ ( 4 ) ) ( q ^ ( 2 ) r zeta ^ ( 2 ) ) ) zeta ^ ( - 2 ) ,\n",
      "Encoded:   [1, 691, 2864, 2662, 3044, 1009, 2899, 1035, 2823, 921, 2999, 2907, 681, 2823, 1579, 3586, 2823, 691, 2864, 2662, 3044, 681, 2899, 1035, 2823, 921, 2999, 2907, 691, 2861, 1035, 2823, 921, 2999, 2985, 2107, 2864, 3273, 1835, 2899, 2687, 2823, 1035, 2866, 2395, 2871, 2107, 2864, 3273, 1835, 2899, 3273, 2395, 3178, 2823, 1035, 2866, 707, 2823, 2395, 3074, 2107, 2864, 3273, 1835, 2899, 2687, 2823, 1035, 2866, 707, 2823, 2395, 2871, 2107, 2864, 3273, 1835, 2899, 3273, 2395, 3178, 2823, 1035, 2866, 2395, 2991, 1035, 2866, 707, 2823, 2395, 2862, 1579, 2]\n",
      "Decoded:   Lambda _ ( W ) ^ ( ( 0 ) ) ( zeta ; r ) = 1 , wide Lambda _ ( W ) ^ ( ( 1 ) ) ( zeta ; r ) = Lambda ( zeta ; r ) = ( Theta _ ( q ^ ( 4 ) ) ( r zeta ^ ( 2 ) ) Theta _ ( q ^ ( 4 ) ) ( q ^ ( 2 ) r zeta ^ ( - 2 ) ) ) / ( Theta _ ( q ^ ( 4 ) ) ( r zeta ^ ( - 2 ) ) Theta _ ( q ^ ( 4 ) ) ( q ^ ( 2 ) r zeta ^ ( 2 ) ) ) zeta ^ ( - 2 ) ,\n",
      "\n",
      "Original:  hat( delta ) overline( eta ) ^ ( a ) = - partial _ ( mu ) A ^ ( a mu ) delta sigma.alt\n",
      "Encoded:   [1, 1757, 2859, 779, 2862, 647, 2859, 1495, 3161, 2907, 707, 2823, 829, 2864, 2631, 3265, 3129, 2823, 2631, 2862, 779, 2823, 555, 2]\n",
      "Decoded:   hat( delta ) overline( eta ) ^ ( a ) = - partial _ ( mu ) A ^ ( a mu ) delta sigma.alt\n",
      "\n",
      "Original:  lim _ ( w -> 9 ^ ( + ) ) ( 5 + - tan ^ ( 0 ) w ) / ( tan w + w cos ^ ( 2 ) w )\n",
      "Encoded:   [1, 1141, 2864, 2691, 2823, 707, 749, 2823, 1451, 2866, 31, 2899, 1961, 2823, 31, 2823, 707, 2823, 1453, 2866, 1009, 3845, 1453, 3051, 2823, 31, 3051, 2823, 1279, 2866, 2395, 3713, 2]\n",
      "Decoded:   lim _ ( w -> 9 ^ ( + ) ) ( 5 + - tan ^ ( 0 ) w ) / ( tan w + w cos ^ ( 2 ) w )\n",
      "\n",
      "Original:  lim _ ( s -> oo ) ( 92 ln s ) / ( sqrt( x ) )\n",
      "Encoded:   [1, 1141, 3235, 2823, 707, 749, 2823, 609, 2869, 1451, 2395, 2823, 63, 2870, 2868, 2373, 2859, 3618, 2]\n",
      "Decoded:   lim _ ( s -> oo ) ( 92 ln s ) / ( sqrt( x ) )\n",
      "\n",
      "Original:  lim _ ( v -> 2 ) ( 9 + tan v ) / ( 8 + - 5 cos ^ ( 2 ) v )\n",
      "Encoded:   [1, 1141, 2864, 2690, 2823, 707, 749, 2823, 2395, 2869, 1451, 2823, 31, 2823, 1453, 3130, 2868, 801, 2823, 31, 2823, 707, 2823, 1961, 2823, 1279, 2866, 2395, 3187, 2860, 2]\n",
      "Decoded:   lim _ ( v -> 2 ) ( 9 + tan v ) / ( 8 + - 5 cos ^ ( 2 ) v )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test it out on some examples\n",
    "for i in range(5):\n",
    "    example = ds[i]\n",
    "    encoded = tokenizer.encode(example)\n",
    "    decoded = tokenizer.decode(encoded.ids, skip_special_tokens=True)\n",
    "    print(\"Original: \", example)\n",
    "    print(\"Encoded:  \", encoded.ids)\n",
    "    print(\"Decoded:  \", decoded)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7255dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Wrap the raw tokenizer\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=ARCHITECTURE_TOKENS[\"bos_token\"],\n",
    "    eos_token=ARCHITECTURE_TOKENS[\"eos_token\"],\n",
    "    unk_token=ARCHITECTURE_TOKENS[\"unk_token\"],\n",
    "    pad_token=ARCHITECTURE_TOKENS[\"pad_token\"],\n",
    "    mask_token=ARCHITECTURE_TOKENS[\"mask_token\"],\n",
    "    cls_token=ARCHITECTURE_TOKENS[\"bos_token\"],\n",
    "    sep_token=ARCHITECTURE_TOKENS[\"eos_token\"],\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cebf738f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['underbrace', '(', 'f', '(', 'theta', ')', ',', 'Ġ\"', 'o', 'b', 'j', '\"', ')']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_TEXT = \"\"\"underbrace(f(theta), \"obj\")\"\"\"\n",
    "\n",
    "wrapped_tokenizer.tokenize(SAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49701cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Lambda _ ( W ) ^ ( ( 0 ) ) ( zeta ; r ) = 1 , wide Lambda _ ( W ) ^ ( ( 1 ) ) ( zeta ; r ) = Lambda ( zeta ; r ) = ( Theta _ ( q ^ ( 4 ) ) ( r zeta ^ ( 2 ) ) Theta _ ( q ^ ( 4 ) ) ( q ^ ( 2 ) r zeta ^ ( - 2 ) ) ) / ( Theta _ ( q ^ ( 4 ) ) ( r zeta ^ ( - 2 ) ) Theta _ ( q ^ ( 4 ) ) ( q ^ ( 2 ) r zeta ^ ( 2 ) ) ) zeta ^ ( - 2 ) ,\n",
      "Encoded:   [1, 691, 2864, 2662, 3044, 1009, 2899, 1035, 2823, 921, 2999, 2907, 681, 2823, 1579, 3586, 2823, 691, 2864, 2662, 3044, 681, 2899, 1035, 2823, 921, 2999, 2907, 691, 2861, 1035, 2823, 921, 2999, 2985, 2107, 2864, 3273, 1835, 2899, 2687, 2823, 1035, 2866, 2395, 2871, 2107, 2864, 3273, 1835, 2899, 3273, 2395, 3178, 2823, 1035, 2866, 707, 2823, 2395, 3074, 2107, 2864, 3273, 1835, 2899, 2687, 2823, 1035, 2866, 707, 2823, 2395, 2871, 2107, 2864, 3273, 1835, 2899, 3273, 2395, 3178, 2823, 1035, 2866, 2395, 2991, 1035, 2866, 707, 2823, 2395, 2862, 1579, 2]\n",
      "Tokenized:  ['Lambda', 'Ġ_Ġ(Ġ', 'W', 'Ġ)Ġ^Ġ(Ġ(Ġ', '0', 'Ġ)Ġ)Ġ(Ġ', 'zeta', 'Ġ', ';', 'Ġr', 'Ġ)Ġ=Ġ', '1', 'Ġ', ',', 'Ġwide', 'Ġ', 'Lambda', 'Ġ_Ġ(Ġ', 'W', 'Ġ)Ġ^Ġ(Ġ(Ġ', '1', 'Ġ)Ġ)Ġ(Ġ', 'zeta', 'Ġ', ';', 'Ġr', 'Ġ)Ġ=Ġ', 'Lambda', 'Ġ(Ġ', 'zeta', 'Ġ', ';', 'Ġr', 'Ġ)Ġ=Ġ(Ġ', 'Theta', 'Ġ_Ġ(Ġ', 'qĠ^Ġ(Ġ', '4', 'Ġ)Ġ)Ġ(Ġ', 'r', 'Ġ', 'zeta', 'Ġ^Ġ(Ġ', '2', 'Ġ)Ġ)Ġ', 'Theta', 'Ġ_Ġ(Ġ', 'qĠ^Ġ(Ġ', '4', 'Ġ)Ġ)Ġ(Ġ', 'qĠ^Ġ(Ġ', '2', 'Ġ)Ġr', 'Ġ', 'zeta', 'Ġ^Ġ(Ġ', '-', 'Ġ', '2', 'Ġ)Ġ)Ġ)Ġ/Ġ(Ġ', 'Theta', 'Ġ_Ġ(Ġ', 'qĠ^Ġ(Ġ', '4', 'Ġ)Ġ)Ġ(Ġ', 'r', 'Ġ', 'zeta', 'Ġ^Ġ(Ġ', '-', 'Ġ', '2', 'Ġ)Ġ)Ġ', 'Theta', 'Ġ_Ġ(Ġ', 'qĠ^Ġ(Ġ', '4', 'Ġ)Ġ)Ġ(Ġ', 'qĠ^Ġ(Ġ', '2', 'Ġ)Ġr', 'Ġ', 'zeta', 'Ġ^Ġ(Ġ', '2', 'Ġ)Ġ)Ġ)Ġ', 'zeta', 'Ġ^Ġ(Ġ', '-', 'Ġ', '2', 'Ġ)Ġ', ',']\n",
      "Decoded:   Lambda _ ( W ) ^ ( ( 0 ) ) ( zeta ; r ) = 1 , wide Lambda _ ( W ) ^ ( ( 1 ) ) ( zeta ; r ) = Lambda ( zeta ; r ) = ( Theta _ ( q ^ ( 4 ) ) ( r zeta ^ ( 2 ) ) Theta _ ( q ^ ( 4 ) ) ( q ^ ( 2 ) r zeta ^ ( - 2 ) ) ) / ( Theta _ ( q ^ ( 4 ) ) ( r zeta ^ ( - 2 ) ) Theta _ ( q ^ ( 4 ) ) ( q ^ ( 2 ) r zeta ^ ( 2 ) ) ) zeta ^ ( - 2 ) ,\n",
      "\n",
      "Original:  hat( delta ) overline( eta ) ^ ( a ) = - partial _ ( mu ) A ^ ( a mu ) delta sigma.alt\n",
      "Encoded:   [1, 1757, 2859, 779, 2862, 647, 2859, 1495, 3161, 2907, 707, 2823, 829, 2864, 2631, 3265, 3129, 2823, 2631, 2862, 779, 2823, 555, 2]\n",
      "Tokenized:  ['hat', '(Ġ', 'delta', 'Ġ)Ġ', 'overline', '(Ġ', 'eta', 'Ġ)Ġ^Ġ(Ġa', 'Ġ)Ġ=Ġ', '-', 'Ġ', 'partial', 'Ġ_Ġ(Ġ', 'mu', 'Ġ)ĠA', 'Ġ^Ġ(Ġa', 'Ġ', 'mu', 'Ġ)Ġ', 'delta', 'Ġ', 'sigma.alt']\n",
      "Decoded:   hat( delta ) overline( eta ) ^ ( a ) = - partial _ ( mu ) A ^ ( a mu ) delta sigma.alt\n",
      "\n",
      "Original:  lim _ ( w -> 9 ^ ( + ) ) ( 5 + - tan ^ ( 0 ) w ) / ( tan w + w cos ^ ( 2 ) w )\n",
      "Encoded:   [1, 1141, 2864, 2691, 2823, 707, 749, 2823, 1451, 2866, 31, 2899, 1961, 2823, 31, 2823, 707, 2823, 1453, 2866, 1009, 3845, 1453, 3051, 2823, 31, 3051, 2823, 1279, 2866, 2395, 3713, 2]\n",
      "Tokenized:  ['lim', 'Ġ_Ġ(Ġ', 'w', 'Ġ', '-', '>', 'Ġ', '9', 'Ġ^Ġ(Ġ', '+', 'Ġ)Ġ)Ġ(Ġ', '5', 'Ġ', '+', 'Ġ', '-', 'Ġ', 'tan', 'Ġ^Ġ(Ġ', '0', 'Ġ)ĠwĠ)Ġ/Ġ(Ġ', 'tan', 'Ġw', 'Ġ', '+', 'Ġw', 'Ġ', 'cos', 'Ġ^Ġ(Ġ', '2', 'Ġ)ĠwĠ)']\n",
      "Decoded:   lim _ ( w -> 9 ^ ( + ) ) ( 5 + - tan ^ ( 0 ) w ) / ( tan w + w cos ^ ( 2 ) w )\n",
      "\n",
      "Original:  lim _ ( s -> oo ) ( 92 ln s ) / ( sqrt( x ) )\n",
      "Encoded:   [1, 1141, 3235, 2823, 707, 749, 2823, 609, 2869, 1451, 2395, 2823, 63, 2870, 2868, 2373, 2859, 3618, 2]\n",
      "Tokenized:  ['lim', 'Ġ_Ġ(Ġs', 'Ġ', '-', '>', 'Ġ', 'oo', 'Ġ)Ġ(Ġ', '9', '2', 'Ġ', 'ln', 'Ġs', 'Ġ)Ġ/Ġ(Ġ', 'sqrt', '(Ġ', 'xĠ)Ġ)']\n",
      "Decoded:   lim _ ( s -> oo ) ( 92 ln s ) / ( sqrt( x ) )\n",
      "\n",
      "Original:  lim _ ( v -> 2 ) ( 9 + tan v ) / ( 8 + - 5 cos ^ ( 2 ) v )\n",
      "Encoded:   [1, 1141, 2864, 2690, 2823, 707, 749, 2823, 2395, 2869, 1451, 2823, 31, 2823, 1453, 3130, 2868, 801, 2823, 31, 2823, 707, 2823, 1961, 2823, 1279, 2866, 2395, 3187, 2860, 2]\n",
      "Tokenized:  ['lim', 'Ġ_Ġ(Ġ', 'v', 'Ġ', '-', '>', 'Ġ', '2', 'Ġ)Ġ(Ġ', '9', 'Ġ', '+', 'Ġ', 'tan', 'Ġv', 'Ġ)Ġ/Ġ(Ġ', '8', 'Ġ', '+', 'Ġ', '-', 'Ġ', '5', 'Ġ', 'cos', 'Ġ^Ġ(Ġ', '2', 'Ġ)Ġv', 'Ġ)']\n",
      "Decoded:   lim _ ( v -> 2 ) ( 9 + tan v ) / ( 8 + - 5 cos ^ ( 2 ) v )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    example = ds[i]\n",
    "    encoded = tokenizer.encode(example)\n",
    "    tokenized = wrapped_tokenizer.tokenize(example)\n",
    "    decoded = tokenizer.decode(encoded.ids, skip_special_tokens=True)\n",
    "    print(\"Original: \", example)\n",
    "    print(\"Encoded:  \", encoded.ids)\n",
    "    print(\"Tokenized: \", tokenized)\n",
    "    print(\"Decoded:  \", decoded)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2a933b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/jkl/Code/DTU-02456-Deep-Learning-Project/models/tokenizers/typst_tokenizer/tokenizer_config.json',\n",
       " '/home/jkl/Code/DTU-02456-Deep-Learning-Project/models/tokenizers/typst_tokenizer/special_tokens_map.json',\n",
       " '/home/jkl/Code/DTU-02456-Deep-Learning-Project/models/tokenizers/typst_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.save_pretrained(TOKENIZER_MODELS_DIR / \"typst_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30465ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapped_tokenizer.push_to_hub(\"JeppeKlitgaard/typst-tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
