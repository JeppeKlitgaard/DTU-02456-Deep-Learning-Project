{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adefbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import PIL.Image as pil_image\n",
    "import torch\n",
    "import typst\n",
    "from datasets import ClassLabel, DatasetDict, Features, Image, Value, load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from joblib import Parallel, delayed\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    TrOCRForCausalLM,\n",
    "    TrOCRProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "from typstscribe.const import DATA_DIR, LOGS_DIR, MODULE_DIR, PROJECT_DIR, MODELS_DIR, TOKENIZER_MODELS_DIR\n",
    "from typstscribe.image import convert_to_rgb_white_bg\n",
    "from typstscribe.typst import TypstGrayscaleCompiler\n",
    "\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5001f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL = \"microsoft/trocr-small-stage1\"  # We use stage1 instead of printed for better generalization\n",
    "TEXT_COLUMN = \"typst\"  # Or \"latex\" if we wanted to do LaTeX OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360a988",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e94664c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fe48b17dfa4eca8f04b0cfd7ba5daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 354 files:   0%|          | 0/354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jkl/.cache/huggingface/hub/datasets--JeppeKlitgaard--typst-image-dataset/snapshots/e855cf9b802ae2ac3b962f36f76f34977bd94155')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save a local copy of the entire dataset\n",
    "dataset_path = Path(\n",
    "    snapshot_download(\n",
    "        repo_id=\"JeppeKlitgaard/typst-image-dataset\",\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    ")\n",
    "display(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b4660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dacd6df18a944e89ec33cf820b1e2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db60703df862478aad128434e14ac8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'latex', 'typst'],\n",
       "        num_rows: 943190\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'latex', 'typst'],\n",
       "        num_rows: 1366\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features and do basic filtering\n",
    "# We only keep printed images for this model\n",
    "features = Features(\n",
    "    {\n",
    "        \"image\": Image(),\n",
    "        \"latex.txt\": Value(\"string\"),\n",
    "        \"typst.txt\": Value(\"string\"),\n",
    "        \"metadata.json\": {\n",
    "            \"image_type\": ClassLabel(names=[\"handwritten\", \"printed\"]),\n",
    "            \"image_extension\": ClassLabel(names=[\"png\", \"jpg\", \"bmp\", \"dvi\"]),\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"JeppeKlitgaard/typst-image-dataset\",\n",
    "    data_files={\n",
    "        # \"test\": \"test/*.tar\",\n",
    "        # \"train\": \"train/shard_0001*.tar\",\n",
    "        \"train\": \"train/*.tar\",\n",
    "        \"validation\": \"validation/*.tar\",\n",
    "    },\n",
    "    # Maybe numproc here?\n",
    "    features=features,\n",
    ")\n",
    "ds = ds.flatten()\n",
    "ds = ds.rename_columns(\n",
    "    {\n",
    "        \"typst.txt\": \"typst\",\n",
    "        \"latex.txt\": \"latex\",\n",
    "        \"metadata.json.image_type\": \"image_type\",\n",
    "        \"metadata.json.image_extension\": \"image_extension\",\n",
    "    }\n",
    ")\n",
    "ds = ds.remove_columns([\"image_extension\"])\n",
    "\n",
    "_target_image_type_int = features.flatten()[\"metadata.json.image_type\"].str2int(\n",
    "    \"printed\"\n",
    ")\n",
    "\n",
    "\n",
    "def filter_image_type(image_type_int):\n",
    "    return image_type_int == _target_image_type_int\n",
    "\n",
    "\n",
    "ds = ds.filter(\n",
    "    filter_image_type,\n",
    "    input_columns=[\"image_type\"],\n",
    ")  # Filter does not hydrate the class label apparently\n",
    "ds = ds.remove_columns([\"image_type\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c881bdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=328x86>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'G _ ( oo ) = G _ ( 0 ) ( 1 + sqrt( ( M _ ( 0 ) ) / ( M ) ) ) ,'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=328x86>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The HuggingFace Datasets WebDataset loader is a bit wonky to work with, so just validate that\n",
    "# We are getting the correct data types by having a look here:\n",
    "display(ds[\"train\"][\"image\"][0].convert(\"RGB\"))\n",
    "display(ds[\"train\"][\"typst\"][0])\n",
    "\n",
    "display(convert_to_rgb_white_bg(ds[\"train\"][\"image\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19c74b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = DatasetDict(\n",
    "#     {\n",
    "#         \"train\": ds[\"train\"].take(400_000),\n",
    "#         # \"train\": ds[\"train\"].take(100_000),\n",
    "#         # \"train\": ds[\"train\"].take(10_000),\n",
    "#         \"validation\": ds[\"validation\"].take(1_000),\n",
    "#     }\n",
    "# )\n",
    "# ds\n",
    "\n",
    "# ds = ds.to_iterable_dataset(num_shards=256)\n",
    "ds_train = ds[\"train\"].to_iterable_dataset(num_shards=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30fc1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessorSetup(Enum):\n",
    "    TYPST_TOKENIZER = \"typst_tokenizer\"\n",
    "    DEFAULT= \"default\"\n",
    "\n",
    "PROCESSOR_SETUP = ProcessorSetup.TYPST_TOKENIZER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5475f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processor\n",
    "\n",
    "match PROCESSOR_SETUP:\n",
    "    case ProcessorSetup.TYPST_TOKENIZER:\n",
    "        # Uncomment to use HuggingFace hosted tokenizer instead of local development version\n",
    "        # Local copy of tokenizer\n",
    "        # tokenizer_path = \"JeppeKlitgaard/typst-tokenizer\"\n",
    "        # snapshot_download(repo_id=\"JeppeKlitgaard/typst-tokenizer\", repo_type=\"model\")\n",
    "\n",
    "        tokenizer_path = TOKENIZER_MODELS_DIR / \"typst_tokenizer\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # Use existing image processor from base model\n",
    "        image_processor = AutoImageProcessor.from_pretrained(PRETRAINED_MODEL, use_fast=True)\n",
    "\n",
    "        # Fix for when parallelising the data loaders as well\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "        processor = TrOCRProcessor(\n",
    "            image_processor=image_processor, tokenizer=tokenizer, use_fast=True\n",
    "        )\n",
    "\n",
    "    case ProcessorSetup.DEFAULT:\n",
    "        processor = TrOCRProcessor.from_pretrained(PRETRAINED_MODEL, use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1facb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### Model\n",
    "\n",
    "MAX_LENGTH = 256  # Sensible, see typst_tokenizer.ipynb\n",
    "# MAX_LENGTH = (\n",
    "#     128  # Smaller since we mostly want to prove that the architecture could work\n",
    "# )\n",
    "\n",
    "### Model\n",
    "pretrained_model = VisionEncoderDecoderModel.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "encoder = pretrained_model.encoder\n",
    "decoder = pretrained_model.decoder\n",
    "\n",
    "decoder.config.vocab_size = len(processor.tokenizer)\n",
    "decoder.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "decoder.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "decoder.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "decoder.config.bos_token_id = processor.tokenizer.cls_token_id\n",
    "\n",
    "model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "# Definitely some redundant assignments here, but I am afraid of breaking something\n",
    "\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "model.generation_config.max_length = MAX_LENGTH\n",
    "model.generation_config.early_stopping = True\n",
    "model.generation_config.no_repeat_ngram_size = 0\n",
    "# length_penalty: https://huggingface.co/docs/transformers/en/internal/generation_utils\n",
    "# length_penalty > 0.0: longer sequences are favoured\n",
    "# length_penalty < 0.0: shorter sequences are favoured\n",
    "# model.generation_config.length_penalty = 2.0  # Don't set this for now, this matches pix2text-mfr-1.5\n",
    "model.generation_config.length_penalty = 1.2  # Slightly higher than default 1.0\n",
    "model.generation_config.repetition_penalty = 1.15  # Disincentivize repetition a bit\n",
    "model.generation_config.num_beams = 4\n",
    "model.generation_config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.generation_config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.generation_config.bos_token_id = processor.tokenizer.cls_token_id\n",
    "model.generation_config.decoder_start_token_id = processor.tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1382de94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrOCRConfig {\n",
       "  \"activation_dropout\": 0.0,\n",
       "  \"activation_function\": \"relu\",\n",
       "  \"add_cross_attention\": true,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"cross_attention_hidden_size\": 384,\n",
       "  \"d_model\": 256,\n",
       "  \"decoder_attention_heads\": 8,\n",
       "  \"decoder_ffn_dim\": 1024,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 1,\n",
       "  \"dropout\": 0.1,\n",
       "  \"dtype\": \"float32\",\n",
       "  \"eos_token_id\": 2,\n",
       "  \"init_std\": 0.02,\n",
       "  \"is_decoder\": true,\n",
       "  \"layernorm_embedding\": true,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"trocr\",\n",
       "  \"pad_token_id\": 0,\n",
       "  \"scale_embedding\": true,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.57.1\",\n",
       "  \"use_cache\": false,\n",
       "  \"use_learned_position_embeddings\": true,\n",
       "  \"vocab_size\": 4096\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6148b267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeiTConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"dtype\": \"float32\",\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 384,\n",
       "  \"image_size\": 384,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"deit\",\n",
       "  \"num_attention_heads\": 6,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"pooler_act\": \"tanh\",\n",
       "  \"pooler_output_size\": 384,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.57.1\"\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69d258d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeiTModel(\n",
       "  (embeddings): DeiTEmbeddings(\n",
       "    (patch_embeddings): DeiTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): DeiTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DeiTLayer(\n",
       "        (attention): DeiTAttention(\n",
       "          (attention): DeiTSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (output): DeiTSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DeiTIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DeiTOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): DeiTPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d0bcc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "typst_compiler = TypstGrayscaleCompiler(ppi=100.0)\n",
    "\n",
    "\n",
    "def preprocess_function(examples, render_typst: bool = False):\n",
    "    N = len(examples[\"image\"])\n",
    "    images = [convert_to_rgb_white_bg(image) for image in examples[\"image\"]]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Tokenize text\n",
    "    labels = processor.tokenizer(\n",
    "        examples[TEXT_COLUMN],\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "\n",
    "    # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "    labels_with_ignore_index = [\n",
    "        [\n",
    "            label if label != processor.tokenizer.pad_token_id else -100\n",
    "            for label in label_example\n",
    "        ]\n",
    "        for label_example in labels\n",
    "    ]\n",
    "\n",
    "    if render_typst:\n",
    "        typst_image = []\n",
    "        is_good = [True] * N\n",
    "        for i, typst_src in enumerate(examples[TEXT_COLUMN]):\n",
    "            try:\n",
    "                typst_image.append(typst_compiler.compile(typst_src))\n",
    "            except Exception:\n",
    "                print(\"GOT ERROR!\")\n",
    "                is_good[i] = False\n",
    "                typst_image.append(None)\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels_with_ignore_index,\n",
    "            \"typst_image\": typst_image,\n",
    "            \"is_good\": is_good,\n",
    "        }\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels_with_ignore_index}\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "# We remove original columns to save memory and prevent collation errors\n",
    "# num_proc = None --> 128 examples/s\n",
    "column_names = ds[\"train\"].column_names\n",
    "ds_train = ds_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    # num_proc=4,  # Setting this above 4 is very memory intensive!\n",
    "    remove_columns=column_names,\n",
    "    # new_fingerprint=\"trocr_train_preprocess_v3_no_tokenizer\",\n",
    ")\n",
    "eval_dataset = ds[\"validation\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=None,  # Setting this to an int seems not to work\n",
    "    remove_columns=column_names,\n",
    "    fn_kwargs={\"render_typst\": True},\n",
    "    new_fingerprint=\"trocr_validation_preprocess_v4\",\n",
    ")\n",
    "\n",
    "\n",
    "# Filter out any samples that have too many tokens\n",
    "def filter_max_length(examples):\n",
    "    label_array = np.array(examples[\"labels\"])\n",
    "    real_lengths = (label_array != -100).sum(axis=1)\n",
    "    return real_lengths <= MAX_LENGTH\n",
    "\n",
    "# Filter out eval samples where Typst rendering failed\n",
    "def filter_typst_good(examples):\n",
    "    is_good_array = np.array(examples[\"is_good\"])\n",
    "    return is_good_array\n",
    "\n",
    "SHORT_SAMPLE_MAX_LENGTH = 100\n",
    "def filter_short_samples_batched(examples):\n",
    "    label_array = np.array(examples[\"labels\"])\n",
    "    real_lengths = (label_array != -100).sum(axis=1)\n",
    "    return real_lengths <= SHORT_SAMPLE_MAX_LENGTH\n",
    "\n",
    "# Apply the vectorized filter\n",
    "ds_train = ds_train.filter(filter_max_length, batched=True)\n",
    "ds_train_short = ds_train.filter(filter_short_samples_batched, batched=True)\n",
    "eval_dataset = eval_dataset.filter(filter_max_length, batched=True)\n",
    "\n",
    "eval_dataset = eval_dataset.filter(filter_typst_good, batched=True)\n",
    "\n",
    "eval_dataset = eval_dataset.remove_columns([\"is_good\"])\n",
    "\n",
    "# Lastly, cut down eval set because evals are expensive\n",
    "eval_dataset = eval_dataset.select(range(128))\n",
    "eval_small_dataset = eval_dataset.select(range(8))\n",
    "\n",
    "# Set format for PyTorch\n",
    "# ds_train = ds_train.select_columns([\"pixel_values\", \"labels\"])\n",
    "ds_train.with_format(type=\"torch\")\n",
    "ds_train_short = ds_train_short.select_columns([\"pixel_values\", \"labels\"])\n",
    "ds_train_short.with_format(type=\"torch\")\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"], output_all_columns=True)  # Do not convert typst_image column\n",
    "eval_small_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04de83d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pixel_values', 'labels', 'typst_image'],\n",
       "    num_rows: 128\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fedf0ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IterableDataset({\n",
       "    features: Unknown,\n",
       "    num_shards: 256\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b151d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 18 CPU cores for parallel processing.\n"
     ]
    }
   ],
   "source": [
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "\n",
    "def calculate_iou(img_pred, img_gt):\n",
    "    \"\"\"\n",
    "    Calculates IoU by placing images on a shared white canvas\n",
    "    without resizing (scaling) them. Aligns images to the top-left.\n",
    "\n",
    "    IoU is okay here because we are generating the images in the exact same manner.\n",
    "    \"\"\"\n",
    "    img_pred = img_pred.convert(\"L\")\n",
    "    img_gt = img_gt.convert(\"L\")\n",
    "\n",
    "    # Get largest dimension for us to copy onto\n",
    "    max_w = max(img_pred.width, img_gt.width)\n",
    "    max_h = max(img_pred.height, img_gt.height)\n",
    "\n",
    "    # White background\n",
    "    canvas_pred = pil_image.new(\"L\", (max_w, max_h), color=255)\n",
    "    canvas_gt = pil_image.new(\"L\", (max_w, max_h), color=255)\n",
    "\n",
    "    canvas_pred.paste(img_pred, (0, 0))\n",
    "    canvas_gt.paste(img_gt, (0, 0))\n",
    "\n",
    "    # Binarize\n",
    "    arr_pred = np.array(canvas_pred) < 128\n",
    "    arr_gt = np.array(canvas_gt) < 128\n",
    "\n",
    "    # IoU\n",
    "    intersection = np.logical_and(arr_pred, arr_gt).sum()\n",
    "    union = np.logical_or(arr_pred, arr_gt).sum()\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def visual_metrics_batch(batch_samples: list[tuple[str, pil_image.Image]]) -> list[tuple[bool, float]]:\n",
    "    \"\"\"\n",
    "    Worker function to process a batch of samples.\n",
    "    Initializes the compiler once per batch to reduce overhead.\n",
    "\n",
    "    Args:\n",
    "        batch_samples: A list of tuples, where each tuple contains (predicted_text, ground_truth_image).\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples (is_compilable, iou_score) corresponding to the input batch.\n",
    "    \"\"\"\n",
    "    local_compiler = TypstGrayscaleCompiler(ppi=100.0)\n",
    "    batch_results = []\n",
    "\n",
    "    for pred_text, gt_img in batch_samples:\n",
    "        try:\n",
    "            pred_img = local_compiler.compile(pred_text)\n",
    "            iou = calculate_iou(pred_img, gt_img)\n",
    "            batch_results.append((True, iou))\n",
    "        except Exception:\n",
    "            batch_results.append((False, 0.0))\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "num_cpu_cores = os.cpu_count() or 1\n",
    "print(f\"Detected {num_cpu_cores} CPU cores for parallel processing.\")\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # TODO: Pred should already have gt_img column?\n",
    "\n",
    "    # Replace -100 with pad_token_id for decoding\n",
    "    pred_ids[pred_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    ## Visual metrics\n",
    "    # Prepare data for parallel processing\n",
    "    ground_truth_images = eval_dataset[\"typst_image\"]\n",
    "    all_samples = list(zip(pred_str, ground_truth_images))\n",
    "\n",
    "    # Split into batches\n",
    "\n",
    "    batch_size = len(all_samples) // num_cpu_cores + 1\n",
    "    batches = [\n",
    "        all_samples[i : i + batch_size]\n",
    "        for i in range(0, len(all_samples), batch_size)\n",
    "    ]\n",
    "\n",
    "    # Run parallel jobs\n",
    "    batch_results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "        delayed(visual_metrics_batch)(batch) for batch in batches\n",
    "    )\n",
    "\n",
    "    # Flatten results\n",
    "    results = [item for sublist in batch_results for item in sublist]\n",
    "\n",
    "    # Calculate final metrics\n",
    "    compilable_flags = [r[0] for r in results]\n",
    "    iou_scores = [r[1] for r in results]\n",
    "\n",
    "    total_preds = len(pred_str)\n",
    "    ratio_compilable = sum(compilable_flags) / total_preds if total_preds > 0 else 0.0\n",
    "    mean_iou = np.mean(iou_scores) if iou_scores else 0.0\n",
    "\n",
    "    return {\n",
    "        \"cer\": cer,\n",
    "        \"ratio_compilable\": ratio_compilable,\n",
    "        \"iou_scores\": mean_iou,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46333c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogPredictionsCallback(TrainerCallback):\n",
    "    def __init__(self, processor, eval_dataset, model, num_samples=4):\n",
    "        self.processor = processor\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.model = model\n",
    "        self.writer = None\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # Only log on the main process\n",
    "        if not state.is_world_process_zero:\n",
    "            return\n",
    "\n",
    "        # Initialize the SummaryWriter if it doesn't exist\n",
    "        if self.writer is None:\n",
    "            self.writer = SummaryWriter(log_dir=args.logging_dir)\n",
    "\n",
    "        # Move inputs to the model's device\n",
    "        pixel_values = torch.stack([s[\"pixel_values\"] for s in self.eval_dataset]).to(self.model.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(pixel_values)\n",
    "\n",
    "        predictions = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        labels = [s[\"labels\"] for s in self.eval_dataset]\n",
    "        # Handle -100 masking\n",
    "        labels = [\n",
    "            [l if l != -100 else self.processor.tokenizer.pad_token_id for l in label]\n",
    "            for label in labels\n",
    "        ]\n",
    "        ground_truths = self.processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        markdown_table = \"| Index | Ground Truth | Prediction |\\n|---|---|---|\\n\"\n",
    "\n",
    "        for i, (gt, pred) in enumerate(zip(ground_truths, predictions)):\n",
    "            # Escape pipes to preserve table structure\n",
    "            gt = gt.replace(\"|\", \"&#124;\").replace(\"\\n\", \" \")\n",
    "            pred = pred.replace(\"|\", \"&#124;\").replace(\"\\n\", \" \")\n",
    "            markdown_table += f\"| {i} | `{gt}` | `{pred}` |\\n\"\n",
    "\n",
    "        self.writer.add_text(\"Eval/Predictions\", markdown_table, global_step=state.global_step)\n",
    "        self.writer.add_images(\"Eval/Images\", pixel_values.cpu(), global_step=state.global_step)\n",
    "\n",
    "        self.model.train()  # Not sure this is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60274f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSetup(Enum):\n",
    "    TWO_STAGE_FROZEN_ENCODER = \"two_stage_frozen_encoder\"\n",
    "    ONE_STAGE = \"one_stage\"\n",
    "\n",
    "TRAINING_SETUP = TrainingSetup.TWO_STAGE_FROZEN_ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "\n",
    "TB_ROOT_DIR = LOGS_DIR / \"tensorboard\"\n",
    "run_name = f\"run-{PROCESSOR_SETUP.value}-{TRAINING_SETUP.value}-{datetime.now().strftime('%Y-%m-%d_%H-%M')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad491dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fine tuning, train both encoder and decoder\n",
    "\n",
    "if TRAINING_SETUP == TrainingSetup.ONE_STAGE:\n",
    "    current_logging_dir = TB_ROOT_DIR / (run_name + \"no_tokenizer\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 12\n",
    "    EPOCHS = 5\n",
    "    GRADIENT_ACCUMULATION_STEPS = 5\n",
    "    WEIGHT_DECAY = 0.05\n",
    "\n",
    "\n",
    "    EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * 1 # 1 for number of GPUs\n",
    "    MAX_STEPS = len(ds[\"train\"]) * EPOCHS // EFFECTIVE_BATCH_SIZE\n",
    "\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Custom optimizer with different learning rates for encoder and decoder\n",
    "    param_dict_encoder = {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n.startswith(\"encoder\")],\n",
    "        \"lr\": 3e-5,  # Low LR for ViT Encoder\n",
    "    }\n",
    "\n",
    "    param_dict_decoder = {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n.startswith(\"decoder\")],\n",
    "        \"lr\": 5e-5,  # High LR for RoBERTa Decoder\n",
    "    }\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=[param_dict_encoder, param_dict_decoder],\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    #\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./trocr_stage3\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        bf16=True,  # RTX 4090 supports BF16\n",
    "        fp16=False,\n",
    "        max_steps=MAX_STEPS,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        predict_with_generate=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "        save_steps=1000,\n",
    "        logging_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=str(current_logging_dir),\n",
    "        dataloader_num_workers=4,\n",
    "        generation_max_length=MAX_LENGTH,\n",
    "        warmup_ratio=0.10,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        # metric_for_best_model=\"iou_scores\",\n",
    "        # greater_is_better=True,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=4,\n",
    "        early_stopping_threshold=0.0,\n",
    "    )\n",
    "\n",
    "    log_predictions_callback = LogPredictionsCallback(\n",
    "        processor=processor,\n",
    "        eval_dataset=eval_small_dataset,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        processing_class=processor,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        optimizers=(optimizer, None),\n",
    "        callbacks=[\n",
    "            log_predictions_callback,\n",
    "            # early_stopping_callback\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    save_directory = f\"./trocr_stage3_final/{run_name}\"\n",
    "    trainer.save_model(save_directory)\n",
    "    processor.save_pretrained(save_directory)\n",
    "    print(f\"Model saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09621b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='294' max='31439' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  294/31439 00:54 < 1:37:03, 5.35 it/s, Epoch 0.01/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Fine tuning, train both encoder and decoder\n",
    "\n",
    "if TRAINING_SETUP == TrainingSetup.TWO_STAGE_FROZEN_ENCODER:\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 12\n",
    "    EPOCHS_STAGE2 = 2\n",
    "    EPOCHS_STAGE3 = 3\n",
    "    GRADIENT_ACCUMULATION_STEPS = 5\n",
    "    WEIGHT_DECAY = 0.05\n",
    "\n",
    "    EFFECTIVE_BATCH_SIZE = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS * 1 # 1 for number of GPUs\n",
    "    MAX_STEPS_STAGE2 = len(ds[\"train\"]) * EPOCHS_STAGE2 // EFFECTIVE_BATCH_SIZE\n",
    "    MAX_STEPS_STAGE3 = len(ds[\"train\"]) * EPOCHS_STAGE3 // EFFECTIVE_BATCH_SIZE\n",
    "\n",
    "    # Custom optimizer with different learning rates for encoder and decoder\n",
    "    param_dict_encoder = {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n.startswith(\"encoder\")],\n",
    "        \"lr\": 4e-5,  # Lower LR for DeiT Encoder\n",
    "    }\n",
    "\n",
    "    param_dict_decoder = {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n.startswith(\"decoder\")],\n",
    "        \"lr\": 5e-5,  # High LR for RoBERTa Decoder\n",
    "    }\n",
    "\n",
    "    ## STAGE 2: Frozen encoder\n",
    "    run_identifier = run_name + \"_stage2\"\n",
    "    current_logging_dir = TB_ROOT_DIR / run_identifier\n",
    "\n",
    "    # Freeze encoder\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=[param_dict_encoder, param_dict_decoder],\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        betas=(0.9, 0.98),  # Recommended values for RoBERTa\n",
    "    )\n",
    "\n",
    "    # Arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str((MODELS_DIR / run_identifier).resolve()),\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        bf16=True,  # RTX 4090 supports BF16\n",
    "        fp16=False,\n",
    "        max_steps=MAX_STEPS_STAGE2,\n",
    "        num_train_epochs=EPOCHS_STAGE2,\n",
    "        predict_with_generate=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "        save_steps=1000,\n",
    "        logging_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=str(current_logging_dir),\n",
    "        dataloader_num_workers=4,\n",
    "        generation_max_length=MAX_LENGTH,\n",
    "        warmup_ratio=0.10,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        # metric_for_best_model=\"iou_scores\",\n",
    "        # greater_is_better=True,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=4,\n",
    "        early_stopping_threshold=0.0,\n",
    "    )\n",
    "\n",
    "    log_predictions_callback = LogPredictionsCallback(\n",
    "        processor=processor,\n",
    "        eval_dataset=eval_small_dataset,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        processing_class=processor,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        optimizers=(optimizer, None),\n",
    "        callbacks=[\n",
    "            log_predictions_callback,\n",
    "            # early_stopping_callback\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    save_directory = MODELS_DIR / run_identifier\n",
    "    trainer.save_model(save_directory)\n",
    "    processor.save_pretrained(save_directory)\n",
    "    print(f\"Model saved to {save_directory}\")\n",
    "\n",
    "\n",
    "    ## Stage 3: Unfreeze encoder\n",
    "    run_identifier = run_name + \"_stage3\"\n",
    "    current_logging_dir = TB_ROOT_DIR / run_identifier\n",
    "\n",
    "    # Unfreeze encoder\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params=[param_dict_encoder, param_dict_decoder],\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        betas=(0.9, 0.98),  # Recommended values for RoBERTa\n",
    "    )\n",
    "\n",
    "    # Arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str((MODELS_DIR / run_identifier).resolve()),\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        bf16=True,  # RTX 4090 supports BF16\n",
    "        fp16=False,\n",
    "        max_steps=MAX_STEPS_STAGE3,\n",
    "        num_train_epochs=EPOCHS_STAGE3,\n",
    "        predict_with_generate=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "        save_steps=1000,\n",
    "        logging_steps=100,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=str(current_logging_dir),\n",
    "        dataloader_num_workers=4,\n",
    "        generation_max_length=MAX_LENGTH,\n",
    "        warmup_ratio=0.10,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        # metric_for_best_model=\"iou_scores\",\n",
    "        # greater_is_better=True,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=4,\n",
    "        early_stopping_threshold=0.0,\n",
    "    )\n",
    "\n",
    "    log_predictions_callback = LogPredictionsCallback(\n",
    "        processor=processor,\n",
    "        eval_dataset=eval_small_dataset,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        processing_class=processor,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        optimizers=(optimizer, None),\n",
    "        callbacks=[\n",
    "            log_predictions_callback,\n",
    "            # early_stopping_callback\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    save_directory = MODELS_DIR / run_identifier\n",
    "    trainer.save_model(save_directory)\n",
    "    processor.save_pretrained(save_directory)\n",
    "    print(f\"Model saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85d52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: |( ) ( _ 1 < _ eta ( )\n",
      "GT  : | bold( upright( u ) ) | _ ( 1 ) < eta _ ( 4 )\n"
     ]
    }
   ],
   "source": [
    "sample = ds[\"validation\"][0]\n",
    "image = sample[\"image\"].convert(\"RGB\")\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(\n",
    "    model.device\n",
    ")\n",
    "\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"Pred:\", generated_text)\n",
    "print(\"GT  :\", sample[\"typst\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
