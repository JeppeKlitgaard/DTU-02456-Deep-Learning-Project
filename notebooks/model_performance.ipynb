{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dcde6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typstscribe.const import MODELS_DIR\n",
    "import transformers\n",
    "from datasets import Features, Image, Value, ClassLabel, load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "from typstscribe.image import convert_to_rgb_white_bg\n",
    "import numpy as np\n",
    "from typstscribe.typst import TypstGrayscaleCompiler\n",
    "import evaluate\n",
    "\n",
    "import PIL.Image as pil_image\n",
    "import os\n",
    "import torch\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1dd3651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DeiTModel(\n",
       "    (embeddings): DeiTEmbeddings(\n",
       "      (patch_embeddings): DeiTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DeiTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): DeiTPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(64002, 256, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
       "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=256, out_features=64002, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models\n",
    "model_default_path = MODELS_DIR / \"run-default-one_stage-2025-12-04_19-52\"\n",
    "model_custom_path = MODELS_DIR / \"run-2025-12-02_11-40\"\n",
    "model_path = model_default_path\n",
    "# model_path = model_custom_path\n",
    "\n",
    "\n",
    "processor = transformers.TrOCRProcessor.from_pretrained(model_path, use_fast=False)\n",
    "model = transformers.VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0151aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLUMN = \"typst\"  # Or \"latex\" if we wanted to do LaTeX OCR\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8179ec99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f750957a4a4c4ac3ae2d72ad6393973f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 354 files:   0%|          | 0/354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jkl/.cache/huggingface/hub/datasets--JeppeKlitgaard--typst-image-dataset/snapshots/e855cf9b802ae2ac3b962f36f76f34977bd94155')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save a local copy of the entire dataset\n",
    "dataset_path = Path(\n",
    "    snapshot_download(\n",
    "        repo_id=\"JeppeKlitgaard/typst-image-dataset\",\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    ")\n",
    "display(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0e730d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9740876f606400d8e5d76140dc8d522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89385a379ca470192e528398fe4a9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'latex', 'typst'],\n",
       "        num_rows: 943190\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'latex', 'typst'],\n",
       "        num_rows: 1366\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eval dataset\n",
    "features = Features(\n",
    "    {\n",
    "        \"image\": Image(),\n",
    "        \"latex.txt\": Value(\"string\"),\n",
    "        \"typst.txt\": Value(\"string\"),\n",
    "        \"metadata.json\": {\n",
    "            \"image_type\": ClassLabel(names=[\"handwritten\", \"printed\"]),\n",
    "            \"image_extension\": ClassLabel(names=[\"png\", \"jpg\", \"bmp\", \"dvi\"]),\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"JeppeKlitgaard/typst-image-dataset\",\n",
    "    data_files={\n",
    "        # \"test\": \"test/*.tar\",\n",
    "        \"train\": \"train/shard_0001*.tar\",\n",
    "        \"validation\": \"validation/*.tar\",\n",
    "    },\n",
    "    # Maybe numproc here?\n",
    "    features=features,\n",
    ")\n",
    "ds = ds.flatten()\n",
    "ds = ds.rename_columns(\n",
    "    {\n",
    "        \"typst.txt\": \"typst\",\n",
    "        \"latex.txt\": \"latex\",\n",
    "        \"metadata.json.image_type\": \"image_type\",\n",
    "        \"metadata.json.image_extension\": \"image_extension\",\n",
    "    }\n",
    ")\n",
    "ds = ds.remove_columns([\"image_extension\"])\n",
    "\n",
    "_target_image_type_int = features.flatten()[\"metadata.json.image_type\"].str2int(\n",
    "    \"printed\"\n",
    ")\n",
    "\n",
    "\n",
    "def filter_image_type(image_type_int):\n",
    "    return image_type_int == _target_image_type_int\n",
    "\n",
    "\n",
    "ds = ds.filter(\n",
    "    filter_image_type,\n",
    "    input_columns=[\"image_type\"],\n",
    ")  # Filter does not hydrate the class label apparently\n",
    "ds = ds.remove_columns([\"image_type\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "47e841a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dde2f184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb56772d6924c64af744bbde7a2d7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT ERROR!\n",
      "GOT ERROR!\n",
      "GOT ERROR!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d74915db7c44e2a1cd47b334ed281b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38f6852892d47ad9c5697d81f382d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1366 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing the dataset\n",
    "typst_compiler = TypstGrayscaleCompiler(ppi=100.0)\n",
    "\n",
    "\n",
    "def preprocess_function(examples, render_typst: bool = False):\n",
    "    N = len(examples[\"image\"])\n",
    "    images = [convert_to_rgb_white_bg(image) for image in examples[\"image\"]]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Tokenize text\n",
    "    labels = processor.tokenizer(\n",
    "        examples[TEXT_COLUMN],\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "\n",
    "    # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "    labels_with_ignore_index = [\n",
    "        [\n",
    "            label if label != processor.tokenizer.pad_token_id else -100\n",
    "            for label in label_example\n",
    "        ]\n",
    "        for label_example in labels\n",
    "    ]\n",
    "\n",
    "    if render_typst:\n",
    "        typst_image = []\n",
    "        is_good = [True] * N\n",
    "        for i, typst_src in enumerate(examples[TEXT_COLUMN]):\n",
    "            try:\n",
    "                typst_image.append(typst_compiler.compile(typst_src))\n",
    "            except Exception:\n",
    "                print(\"GOT ERROR!\")\n",
    "                is_good[i] = False\n",
    "                typst_image.append(None)\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels_with_ignore_index,\n",
    "            \"typst_image\": typst_image,\n",
    "            \"raw_image\": images,\n",
    "            \"is_good\": is_good,\n",
    "            \"gt_typst\": examples[TEXT_COLUMN]\n",
    "        }\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels_with_ignore_index}\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "# We remove original columns to save memory and prevent collation errors\n",
    "# num_proc = None --> 128 examples/s\n",
    "column_names = ds_eval.column_names\n",
    "ds_eval = ds_eval.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=None,  # Setting this to an int seems not to work\n",
    "    remove_columns=column_names,\n",
    "    fn_kwargs={\"render_typst\": True},\n",
    "    new_fingerprint=\"trocr_validation_preprocess_v7\",\n",
    ")\n",
    "\n",
    "\n",
    "# Filter out any samples that have too many tokens\n",
    "def filter_max_length(examples):\n",
    "    label_array = np.array(examples[\"labels\"])\n",
    "    real_lengths = (label_array != -100).sum(axis=1)\n",
    "    return real_lengths <= MAX_LENGTH\n",
    "\n",
    "# Filter out eval samples where Typst rendering failed\n",
    "def filter_typst_good(examples):\n",
    "    is_good_array = np.array(examples[\"is_good\"])\n",
    "    return is_good_array\n",
    "\n",
    "SHORT_SAMPLE_MAX_LENGTH = 100\n",
    "def filter_short_samples_batched(examples):\n",
    "    label_array = np.array(examples[\"labels\"])\n",
    "    real_lengths = (label_array != -100).sum(axis=1)\n",
    "    return real_lengths <= SHORT_SAMPLE_MAX_LENGTH\n",
    "\n",
    "# Apply the vectorized filter\n",
    "ds_eval = ds_eval.filter(filter_max_length, batched=True)\n",
    "ds_eval = ds_eval.filter(filter_typst_good, batched=True)\n",
    "ds_eval = ds_eval.remove_columns([\"is_good\"])\n",
    "\n",
    "# Lastly, cut down eval set because evals are expensive\n",
    "ds_eval = ds_eval.select(range(128))\n",
    "eval_small_dataset = ds_eval.select(range(8))\n",
    "\n",
    "# Set format for PyTorch\n",
    "# ds_train = ds_train.select_columns([\"pixel_values\", \"labels\"])\n",
    "ds_eval.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"], output_all_columns=True)  # Do not convert typst_image column\n",
    "eval_small_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "582c64d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pixel_values', 'labels', 'typst_image', 'raw_image', 'gt_typst'],\n",
       "    num_rows: 128\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 18 CPU cores for parallel processing.\n"
     ]
    }
   ],
   "source": [
    "# Eval metrics\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "\n",
    "def calculate_iou(img_pred, img_gt):\n",
    "    \"\"\"\n",
    "    Calculates IoU by placing images on a shared white canvas\n",
    "    without resizing (scaling) them. Aligns images to the top-left.\n",
    "\n",
    "    IoU is okay here because we are generating the images in the exact same manner.\n",
    "    \"\"\"\n",
    "    img_pred = img_pred.convert(\"L\")\n",
    "    img_gt = img_gt.convert(\"L\")\n",
    "\n",
    "    # Get largest dimension for us to copy onto\n",
    "    max_w = max(img_pred.width, img_gt.width)\n",
    "    max_h = max(img_pred.height, img_gt.height)\n",
    "\n",
    "    # White background\n",
    "    canvas_pred = pil_image.new(\"L\", (max_w, max_h), color=255)\n",
    "    canvas_gt = pil_image.new(\"L\", (max_w, max_h), color=255)\n",
    "\n",
    "    canvas_pred.paste(img_pred, (0, 0))\n",
    "    canvas_gt.paste(img_gt, (0, 0))\n",
    "\n",
    "    # Binarize\n",
    "    arr_pred = np.array(canvas_pred) < 128\n",
    "    arr_gt = np.array(canvas_gt) < 128\n",
    "\n",
    "    # IoU\n",
    "    intersection = np.logical_and(arr_pred, arr_gt).sum()\n",
    "    union = np.logical_or(arr_pred, arr_gt).sum()\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def visual_metrics_batch(batch_samples: list[tuple[str, pil_image.Image]]) -> list[tuple[bool, float]]:\n",
    "    local_compiler = TypstGrayscaleCompiler(ppi=100.0)\n",
    "    batch_results = []\n",
    "\n",
    "    for pred_text, gt_img in batch_samples:\n",
    "        try:\n",
    "            pred_img = local_compiler.compile(pred_text)\n",
    "            iou = calculate_iou(pred_img, gt_img)\n",
    "            batch_results.append((True, iou))\n",
    "        except Exception:\n",
    "            batch_results.append((False, 0.0))\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "num_cpu_cores = os.cpu_count() or 1\n",
    "print(f\"Detected {num_cpu_cores} CPU cores for parallel processing.\")\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # TODO: Pred should already have gt_img column?\n",
    "\n",
    "    # Replace -100 with pad_token_id for decoding\n",
    "    pred_ids[pred_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    ## Visual metrics\n",
    "    # Prepare data for parallel processing\n",
    "    ground_truth_images = ds_eval[\"typst_image\"]\n",
    "    all_samples = list(zip(pred_str, ground_truth_images))\n",
    "\n",
    "    # Split into batches\n",
    "\n",
    "    batch_size = len(all_samples) // num_cpu_cores + 1\n",
    "    batches = [\n",
    "        all_samples[i : i + batch_size]\n",
    "        for i in range(0, len(all_samples), batch_size)\n",
    "    ]\n",
    "\n",
    "    # Run parallel jobs\n",
    "    batch_results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "        delayed(visual_metrics_batch)(batch) for batch in batches\n",
    "    )\n",
    "\n",
    "    # Flatten results\n",
    "    results = [item for sublist in batch_results for item in sublist]\n",
    "\n",
    "    # Calculate final metrics\n",
    "    compilable_flags = [r[0] for r in results]\n",
    "    iou_scores = [r[1] for r in results]\n",
    "\n",
    "    total_preds = len(pred_str)\n",
    "    ratio_compilable = sum(compilable_flags) / total_preds if total_preds > 0 else 0.0\n",
    "    mean_iou = np.mean(iou_scores) if iou_scores else 0.0\n",
    "\n",
    "    return {\n",
    "        \"cer\": cer,\n",
    "        \"ratio_compilable\": ratio_compilable,\n",
    "        \"iou_scores\": mean_iou,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03bcba41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DeiTModel(\n",
       "    (embeddings): DeiTEmbeddings(\n",
       "      (patch_embeddings): DeiTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DeiTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): DeiTPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(64002, 256, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
       "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=256, out_features=64002, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b6d13990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cer': 0.7466276059408639, 'ratio_compilable': 0.2421875, 'iou_scores': np.float64(0.01812379113484058)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on ds_eval\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "def collate_eval(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = pad_sequence(\n",
    "        [item[\"labels\"] for item in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=processor.tokenizer.pad_token_id,\n",
    "    )\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    typst_image = [item[\"typst_image\"] for item in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"typst_image\": typst_image,}\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    ds_eval,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_eval,\n",
    ")\n",
    "\n",
    "pred_sequences = []\n",
    "label_sequences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values,\n",
    "            max_length=MAX_LENGTH,\n",
    "            num_beams=4,\n",
    "        )\n",
    "        pred_sequences.extend(generated_ids.cpu())\n",
    "        label_sequences.extend(batch[\"labels\"])\n",
    "\n",
    "predictions = pad_sequence(\n",
    "    pred_sequences,\n",
    "    batch_first=True,\n",
    "    padding_value=processor.tokenizer.pad_token_id,\n",
    ").numpy()\n",
    "labels = pad_sequence(\n",
    "    label_sequences,\n",
    "    batch_first=True,\n",
    "    padding_value=-100,\n",
    ").numpy()\n",
    "\n",
    "metrics = compute_metrics(EvalPrediction(predictions=predictions, label_ids=labels))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f4a4d479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|(( ) | ( ) et _ 4\n"
     ]
    }
   ],
   "source": [
    "# Try a single sample\n",
    "sample_example = ds_eval[0]\n",
    "sample_example[\"raw_image\"]\n",
    "sample_example[\"typst_image\"]\n",
    "pixel_values = processor(images=sample_example[\"raw_image\"], return_tensors=\"pt\").pixel_values.to(\n",
    "    model.device\n",
    ")\n",
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generated_text)\n",
    "# with torch.no_grad():\n",
    "#     model.generate(sample_example[\"pixel_values\"].to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0fca981e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| bold( upright( u ) ) | _ ( 1 ) < eta _ ( 4 )'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_eval[\"gt_typst\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5351aeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHcAAAAhCAIAAABsjKM3AAADGUlEQVR4Ae2YW5LiMAxFoWv2Bb0yYGXAymYOc6tVKsuxk2C7Gzr+oIwlS1fHDxJ2x+Pxdrv97dD6Re4gtm/IDxDTdh1av8gdxPYN+dE3/Bb9P4GN8oiNsFHeKI8gMCLHtpc3yq0JXC6X1iFnxfsVe/l+v8N3v993ematkn5zyvD9/PzU+xGIr9drlUgPhz/ZoDpZ/rWCdyPztHN3Pp816K3m9r0d+CKPEoT4cDis0KNKieCnE5mwinY6neaGRU3yghlneoey1Txxi5HN2q9DUrg8n13Lg05C0aejNbOilIXBai1EmGTxsH21GIhkX8ZdtDKC1QRlHZoPmqTn88LOgqhMIU40Y9ICJOPJ1/yNYfhepcPRFmI+F5zi6fLE1NsZyV7rjHu3bP+1KTe5fLNctJG9SfeDH5nf/85nDBjR5mv1nkzUwwOD2mX+B8p7ruvrZDBXCkEc45fFY7XHhNGUlZv09oC1gkJXvtJj144uhOxGlsnWIynETxlNWVJQ4EUk+qpfKYzpFEmnvKGqoaoOVcrZQtgHPvJoypw7tkk8fV5Ttc90foisfkrqx1pZsoKnTIhJNvhoylWC8x3EmmcmdhMN1nYPzg9S9lRAgkc3rWs0CXGyKi9M2SrncIg1O4g/K5qzjihJbSfJZKiDhvjAt55yciiSZOO/ijWqaK1YT6GkuqyJBSZ7rL1EOTtBIfrdg1HiohGxBgFNrJ+RShCyJ8dfeqKJRAxmnUuU/UnxWrVifg10J3qfRWiaO+vKpmYaVaz7eVQ5HoLpzJrIFe8KTSlRRqtU4koytoZaDMcI0Pk0HT+hY6ylfylrlZOlHE3svKynOFTesCU0i4yzScuaftSglSAQ/E7OlEd1gGN69E8os7U1ok/zt6+lvWze79GB2nzEKjmLGJPweWthI+Nf2cvvwbdtFZwJAnqs4PbElY4rFB8d91+0l9uy9pSnItuN8XCAOkepectGJnHUxwjjzQV0CiimU4JjgVQ3+sbgZE097kj9q3zGK0LKswVuN8aaZY3HsRzlsZeXzilH9NZ+kX2WwX0uoqUZ/wHab1d81xfDXAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=119x33>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_eval[0][\"raw_image\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
