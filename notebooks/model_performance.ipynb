{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcde6b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typstscribe.const import MODELS_DIR\n",
    "import transformers\n",
    "from datasets import Features, Image, Value, ClassLabel, load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "from typstscribe.image import convert_to_rgb_white_bg\n",
    "import numpy as np\n",
    "from typstscribe.typst import TypstGrayscaleCompiler\n",
    "import evaluate\n",
    "\n",
    "import PIL.Image as pil_image\n",
    "import os\n",
    "import torch\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1dd3651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DeiTModel(\n",
       "    (embeddings): DeiTEmbeddings(\n",
       "      (patch_embeddings): DeiTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DeiTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): DeiTPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(64002, 256, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
       "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=256, out_features=64002, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load models\n",
    "model_default_path = MODELS_DIR / \"run-default-one_stage-2025-12-04_19-52\"\n",
    "model_custom_path = MODELS_DIR / \"run-2025-12-02_11-40\"\n",
    "model_path = model_default_path\n",
    "# model_path = model_custom_path\n",
    "\n",
    "\n",
    "processor = transformers.TrOCRProcessor.from_pretrained(model_path, use_fast=False)\n",
    "model = transformers.VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0151aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLUMN = \"typst\"  # Or \"latex\" if we wanted to do LaTeX OCR\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8179ec99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d91986f54846869d9cdd93e35b9c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 354 files:   0%|          | 0/354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jkl/.cache/huggingface/hub/datasets--JeppeKlitgaard--typst-image-dataset/snapshots/e855cf9b802ae2ac3b962f36f76f34977bd94155')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save a local copy of the entire dataset\n",
    "dataset_path = Path(\n",
    "    snapshot_download(\n",
    "        repo_id=\"JeppeKlitgaard/typst-image-dataset\",\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    ")\n",
    "display(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e730d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28c8c4b441f4c5888b0ede8cd153a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf058ef68b74d72953196820c727653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'latex', 'typst'],\n",
       "        num_rows: 943190\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'latex', 'typst'],\n",
       "        num_rows: 1366\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eval dataset\n",
    "features = Features(\n",
    "    {\n",
    "        \"image\": Image(),\n",
    "        \"latex.txt\": Value(\"string\"),\n",
    "        \"typst.txt\": Value(\"string\"),\n",
    "        \"metadata.json\": {\n",
    "            \"image_type\": ClassLabel(names=[\"handwritten\", \"printed\"]),\n",
    "            \"image_extension\": ClassLabel(names=[\"png\", \"jpg\", \"bmp\", \"dvi\"]),\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"JeppeKlitgaard/typst-image-dataset\",\n",
    "    data_files={\n",
    "        # \"test\": \"test/*.tar\",\n",
    "        \"train\": \"train/shard_0001*.tar\",\n",
    "        \"validation\": \"validation/*.tar\",\n",
    "    },\n",
    "    # Maybe numproc here?\n",
    "    features=features,\n",
    ")\n",
    "ds = ds.flatten()\n",
    "ds = ds.rename_columns(\n",
    "    {\n",
    "        \"typst.txt\": \"typst\",\n",
    "        \"latex.txt\": \"latex\",\n",
    "        \"metadata.json.image_type\": \"image_type\",\n",
    "        \"metadata.json.image_extension\": \"image_extension\",\n",
    "    }\n",
    ")\n",
    "ds = ds.remove_columns([\"image_extension\"])\n",
    "\n",
    "_target_image_type_int = features.flatten()[\"metadata.json.image_type\"].str2int(\n",
    "    \"printed\"\n",
    ")\n",
    "\n",
    "\n",
    "def filter_image_type(image_type_int):\n",
    "    return image_type_int == _target_image_type_int\n",
    "\n",
    "\n",
    "ds = ds.filter(\n",
    "    filter_image_type,\n",
    "    input_columns=[\"image_type\"],\n",
    ")  # Filter does not hydrate the class label apparently\n",
    "ds = ds.remove_columns([\"image_type\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e841a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dde2f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "typst_compiler = TypstGrayscaleCompiler(ppi=100.0)\n",
    "\n",
    "\n",
    "def preprocess_function(examples, render_typst: bool = False):\n",
    "    N = len(examples[\"image\"])\n",
    "    images = [convert_to_rgb_white_bg(image) for image in examples[\"image\"]]\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Tokenize text\n",
    "    labels = processor.tokenizer(\n",
    "        examples[TEXT_COLUMN],\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "\n",
    "    # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "    labels_with_ignore_index = [\n",
    "        [\n",
    "            label if label != processor.tokenizer.pad_token_id else -100\n",
    "            for label in label_example\n",
    "        ]\n",
    "        for label_example in labels\n",
    "    ]\n",
    "\n",
    "    if render_typst:\n",
    "        typst_image = []\n",
    "        is_good = [True] * N\n",
    "        for i, typst_src in enumerate(examples[TEXT_COLUMN]):\n",
    "            try:\n",
    "                typst_image.append(typst_compiler.compile(typst_src))\n",
    "            except Exception:\n",
    "                print(\"GOT ERROR!\")\n",
    "                is_good[i] = False\n",
    "                typst_image.append(None)\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": labels_with_ignore_index,\n",
    "            \"typst_image\": typst_image,\n",
    "            \"is_good\": is_good,\n",
    "        }\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels_with_ignore_index}\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "# We remove original columns to save memory and prevent collation errors\n",
    "# num_proc = None --> 128 examples/s\n",
    "column_names = ds_eval.column_names\n",
    "ds_eval = ds_eval.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=None,  # Setting this to an int seems not to work\n",
    "    remove_columns=column_names,\n",
    "    fn_kwargs={\"render_typst\": True},\n",
    "    new_fingerprint=\"trocr_validation_preprocess_v5\",\n",
    ")\n",
    "\n",
    "\n",
    "# Filter out any samples that have too many tokens\n",
    "def filter_max_length(examples):\n",
    "    label_array = np.array(examples[\"labels\"])\n",
    "    real_lengths = (label_array != -100).sum(axis=1)\n",
    "    return real_lengths <= MAX_LENGTH\n",
    "\n",
    "# Filter out eval samples where Typst rendering failed\n",
    "def filter_typst_good(examples):\n",
    "    is_good_array = np.array(examples[\"is_good\"])\n",
    "    return is_good_array\n",
    "\n",
    "SHORT_SAMPLE_MAX_LENGTH = 100\n",
    "def filter_short_samples_batched(examples):\n",
    "    label_array = np.array(examples[\"labels\"])\n",
    "    real_lengths = (label_array != -100).sum(axis=1)\n",
    "    return real_lengths <= SHORT_SAMPLE_MAX_LENGTH\n",
    "\n",
    "# Apply the vectorized filter\n",
    "ds_eval = ds_eval.filter(filter_max_length, batched=True)\n",
    "ds_eval = ds_eval.filter(filter_typst_good, batched=True)\n",
    "ds_eval = ds_eval.remove_columns([\"is_good\"])\n",
    "\n",
    "# Lastly, cut down eval set because evals are expensive\n",
    "ds_eval = ds_eval.select(range(128))\n",
    "eval_small_dataset = ds_eval.select(range(8))\n",
    "\n",
    "# Set format for PyTorch\n",
    "# ds_train = ds_train.select_columns([\"pixel_values\", \"labels\"])\n",
    "ds_eval.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"], output_all_columns=True)  # Do not convert typst_image column\n",
    "eval_small_dataset.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7a100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 18 CPU cores for parallel processing.\n"
     ]
    }
   ],
   "source": [
    "# Eval metrics\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "\n",
    "def calculate_iou(img_pred, img_gt):\n",
    "    \"\"\"\n",
    "    Calculates IoU by placing images on a shared white canvas\n",
    "    without resizing (scaling) them. Aligns images to the top-left.\n",
    "\n",
    "    IoU is okay here because we are generating the images in the exact same manner.\n",
    "    \"\"\"\n",
    "    img_pred = img_pred.convert(\"L\")\n",
    "    img_gt = img_gt.convert(\"L\")\n",
    "\n",
    "    # Get largest dimension for us to copy onto\n",
    "    max_w = max(img_pred.width, img_gt.width)\n",
    "    max_h = max(img_pred.height, img_gt.height)\n",
    "\n",
    "    # White background\n",
    "    canvas_pred = pil_image.new(\"L\", (max_w, max_h), color=255)\n",
    "    canvas_gt = pil_image.new(\"L\", (max_w, max_h), color=255)\n",
    "\n",
    "    canvas_pred.paste(img_pred, (0, 0))\n",
    "    canvas_gt.paste(img_gt, (0, 0))\n",
    "\n",
    "    # Binarize\n",
    "    arr_pred = np.array(canvas_pred) < 128\n",
    "    arr_gt = np.array(canvas_gt) < 128\n",
    "\n",
    "    # IoU\n",
    "    intersection = np.logical_and(arr_pred, arr_gt).sum()\n",
    "    union = np.logical_or(arr_pred, arr_gt).sum()\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def visual_metrics_batch(batch_samples: list[tuple[str, pil_image.Image]]) -> list[tuple[bool, float]]:\n",
    "    \"\"\"\n",
    "    Worker function to process a batch of samples.\n",
    "    Initializes the compiler once per batch to reduce overhead.\n",
    "\n",
    "    Args:\n",
    "        batch_samples: A list of tuples, where each tuple contains (predicted_text, ground_truth_image).\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples (is_compilable, iou_score) corresponding to the input batch.\n",
    "    \"\"\"\n",
    "    local_compiler = TypstGrayscaleCompiler(ppi=100.0)\n",
    "    batch_results = []\n",
    "\n",
    "    for pred_text, gt_img in batch_samples:\n",
    "        try:\n",
    "            pred_img = local_compiler.compile(pred_text)\n",
    "            iou = calculate_iou(pred_img, gt_img)\n",
    "            batch_results.append((True, iou))\n",
    "        except Exception:\n",
    "            batch_results.append((False, 0.0))\n",
    "\n",
    "    return batch_results\n",
    "\n",
    "num_cpu_cores = os.cpu_count() or 1\n",
    "print(f\"Detected {num_cpu_cores} CPU cores for parallel processing.\")\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # TODO: Pred should already have gt_img column?\n",
    "\n",
    "    # Replace -100 with pad_token_id for decoding\n",
    "    pred_ids[pred_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    ## Visual metrics\n",
    "    # Prepare data for parallel processing\n",
    "    ground_truth_images = ds_eval[\"typst_image\"]\n",
    "    all_samples = list(zip(pred_str, ground_truth_images))\n",
    "\n",
    "    # Split into batches\n",
    "\n",
    "    batch_size = len(all_samples) // num_cpu_cores + 1\n",
    "    batches = [\n",
    "        all_samples[i : i + batch_size]\n",
    "        for i in range(0, len(all_samples), batch_size)\n",
    "    ]\n",
    "\n",
    "    # Run parallel jobs\n",
    "    batch_results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "        delayed(visual_metrics_batch)(batch) for batch in batches\n",
    "    )\n",
    "\n",
    "    # Flatten results\n",
    "    results = [item for sublist in batch_results for item in sublist]\n",
    "\n",
    "    # Calculate final metrics\n",
    "    compilable_flags = [r[0] for r in results]\n",
    "    iou_scores = [r[1] for r in results]\n",
    "\n",
    "    total_preds = len(pred_str)\n",
    "    ratio_compilable = sum(compilable_flags) / total_preds if total_preds > 0 else 0.0\n",
    "    mean_iou = np.mean(iou_scores) if iou_scores else 0.0\n",
    "\n",
    "    return {\n",
    "        \"cer\": cer,\n",
    "        \"ratio_compilable\": ratio_compilable,\n",
    "        \"iou_scores\": mean_iou,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03bcba41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): DeiTModel(\n",
       "    (embeddings): DeiTEmbeddings(\n",
       "      (patch_embeddings): DeiTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): DeiTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DeiTLayer(\n",
       "          (attention): DeiTAttention(\n",
       "            (attention): DeiTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (output): DeiTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DeiTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DeiTOutput(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): DeiTPooler(\n",
       "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(64002, 256, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
       "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=256, out_features=64002, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6d13990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cer': 0.7466276059408639, 'ratio_compilable': 0.2421875, 'iou_scores': np.float64(0.01812379113484058)}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on ds_eval\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "def collate_eval(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = pad_sequence(\n",
    "        [item[\"labels\"] for item in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=processor.tokenizer.pad_token_id,\n",
    "    )\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    typst_image = [item[\"typst_image\"] for item in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"typst_image\": typst_image}\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    ds_eval,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_eval,\n",
    ")\n",
    "\n",
    "pred_sequences = []\n",
    "label_sequences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values,\n",
    "            max_length=MAX_LENGTH,\n",
    "            num_beams=4,\n",
    "        )\n",
    "        pred_sequences.extend(generated_ids.cpu())\n",
    "        label_sequences.extend(batch[\"labels\"])\n",
    "\n",
    "predictions = pad_sequence(\n",
    "    pred_sequences,\n",
    "    batch_first=True,\n",
    "    padding_value=processor.tokenizer.pad_token_id,\n",
    ").numpy()\n",
    "labels = pad_sequence(\n",
    "    label_sequences,\n",
    "    batch_first=True,\n",
    "    padding_value=-100,\n",
    ").numpy()\n",
    "\n",
    "metrics = compute_metrics(EvalPrediction(predictions=predictions, label_ids=labels))\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
