@software{blecherLukasblecherLaTeXOCR2025,
  title = {Lukas-Blecher/{{LaTeX-OCR}}},
  author = {Blecher, Lukas},
  date = {2025-11-08T13:45:25Z},
  origdate = {2020-12-11T16:35:13Z},
  url = {https://github.com/lukas-blecher/LaTeX-OCR},
  urldate = {2025-11-08},
  abstract = {pix2tex: Using a ViT to convert images of equations into LaTeX code.},
  keywords = {dataset,deep-learning,im2latex,im2markup,im2text,image-processing,image2text,latex,latex-ocr,machine-learning,math-ocr,ocr,python,pytorch,transformer,vision-transformer,vit}
}

@online{dengImagetoMarkupGenerationCoarsetoFine2017,
  title = {Image-to-{{Markup Generation}} with {{Coarse-to-Fine Attention}}},
  author = {Deng, Yuntian and Kanervisto, Anssi and Ling, Jeffrey and Rush, Alexander M.},
  date = {2017-06-13},
  eprint = {1609.04938},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1609.04938},
  url = {http://arxiv.org/abs/1609.04938},
  urldate = {2025-11-08},
  abstract = {We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\6HNIRWMC\\Deng et al. - 2017 - Image-to-Markup Generation with Coarse-to-Fine Attention.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\D2X4QQ8X\\1609.html}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2025-11-08},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\9EAY7ZMR\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\RD4WI586\\2010.html}
}

@online{HoangquoctrungFusionimagetolatexdatasetsDatasets2024,
  title = {Hoang-Quoc-Trung/Fusion-Image-to-Latex-Datasets · {{Datasets}} at {{Hugging Face}}},
  date = {2024-04-17},
  url = {https://huggingface.co/datasets/hoang-quoc-trung/fusion-image-to-latex-datasets},
  urldate = {2025-11-08},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\jeppe\Zotero\storage\FMVRUN6F\fusion-image-to-latex-datasets.html}
}

@online{IntroductionTypstyleDocs,
  title = {Introduction - {{Typstyle Docs}}},
  url = {https://typstyle-rs.github.io/typstyle/},
  urldate = {2025-11-08},
  file = {C:\Users\jeppe\Zotero\storage\CIKR2XJW\typstyle.html}
}

@online{liTrOCRTransformerbasedOptical2022,
  title = {{{TrOCR}}: {{Transformer-based Optical Character Recognition}} with {{Pre-trained Models}}},
  shorttitle = {{{TrOCR}}},
  author = {Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
  date = {2022-09-06},
  eprint = {2109.10282},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.10282},
  url = {http://arxiv.org/abs/2109.10282},
  urldate = {2025-11-08},
  abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at \textbackslash url\{https://aka.ms/trocr\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\K9JI7YZG\\Li et al. - 2022 - TrOCR Transformer-based Optical Character Recognition with Pre-trained Models.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\4HCXCAP5\\2109.html}
}

@online{MitexrsMitexLaTeX,
  title = {Mitex-Rs/Mitex: {{LaTeX}} Support for {{Typst}}, Powered by {{Rust}} and {{WASM}}. {{https://mitex-rs.github.io/mitex/}}},
  url = {https://github.com/mitex-rs/mitex},
  urldate = {2025-11-08},
  file = {C:\Users\jeppe\Zotero\storage\36Z7H7ZV\mitex.html}
}

@software{paran3xusParaN3xusTypress2025,
  title = {{{ParaN3xus}}/Typress},
  author = {ParaN3xus},
  date = {2025-10-28T10:45:49Z},
  origdate = {2024-07-17T11:43:29Z},
  url = {https://github.com/ParaN3xus/typress},
  urldate = {2025-11-08},
  abstract = {Typst Mathematical Expression OCR}
}

@online{ParaN3xusTex2typLaTeX,
  title = {{{ParaN3xus}}/Tex2typ: {{LaTeX}} Math Equations to {{Typst}} Equations Conversion.},
  url = {https://github.com/ParaN3xus/tex2typ},
  urldate = {2025-11-08},
  file = {C:\Users\jeppe\Zotero\storage\HNA5HSDD\tex2typ.html}
}

@software{qwinsiQwinsiTex2typst2025,
  title = {Qwinsi/Tex2typst},
  author = {{qwinsi}},
  date = {2025-11-08T10:33:22Z},
  origdate = {2024-07-15T00:08:06Z},
  url = {https://github.com/qwinsi/tex2typst},
  urldate = {2025-11-08},
  abstract = {JavaScript library for conversion between TeX/LaTeX and Typst math code.},
  keywords = {latex,math-formula,tex,typst}
}

@software{Tex2typPythonCLI,
  title = {Tex2typ: {{A Python CLI}} Tool for Converting {{LaTeX}} Equations into {{Typst}} Equations},
  shorttitle = {Tex2typ},
  url = {https://Niels-Skovgaard-Jensen.github.io/tex2typ/},
  urldate = {2025-11-08},
  version = {0.0.3},
  keywords = {python,Software Development - Libraries - Python Modules},
  file = {C:\Users\jeppe\Zotero\storage\NJCB6ACI\tex2typ.html}
}

@online{TransformersTutorialsTrOCRMaster,
  title = {Transformers-{{Tutorials}}/{{TrOCR}} at Master · {{NielsRogge}}/{{Transformers-Tutorials}}},
  url = {https://github.com/NielsRogge/Transformers-Tutorials/tree/master/TrOCR},
  urldate = {2025-11-08},
  abstract = {This repository contains demos I made with the Transformers library by HuggingFace. - NielsRogge/Transformers-Tutorials},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\jeppe\Zotero\storage\78CSBKNS\TrOCR.html}
}

@online{TypstNewFoundation2025,
  title = {Typst: {{The}} New Foundation for Documents},
  shorttitle = {Typst},
  date = {2025-11-07},
  url = {https://typst.app/},
  urldate = {2025-11-08},
  abstract = {Typst is the new foundation for documents. Sign up now and experience limitless power to write, create, and automate anything that you can fit on a page.},
  langid = {english},
  organization = {Typst}
}
