@software{latexocr,
  title = {Lukas-Blecher/{{LaTeX-OCR}}},
  author = {Blecher, Lukas},
  date = {2025-11-08T13:45:25Z},
  origdate = {2020-12-11T16:35:13Z},
  url = {https://github.com/lukas-blecher/LaTeX-OCR},
  urldate = {2025-11-08},
  abstract = {pix2tex: Using a ViT to convert images of equations into LaTeX code.},
  keywords = {dataset,deep-learning,im2latex,im2markup,im2text,image-processing,image2text,latex,latex-ocr,machine-learning,math-ocr,ocr,python,pytorch,transformer,vision-transformer,vit}
}

@online{dengImagetoMarkupGenerationCoarsetoFine2017,
  title = {Image-to-{{Markup Generation}} with {{Coarse-to-Fine Attention}}},
  author = {Deng, Yuntian and Kanervisto, Anssi and Ling, Jeffrey and Rush, Alexander M.},
  date = {2017-06-13},
  eprint = {1609.04938},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1609.04938},
  url = {http://arxiv.org/abs/1609.04938},
  urldate = {2025-11-08},
  abstract = {We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\6HNIRWMC\\Deng et al. - 2017 - Image-to-Markup Generation with Coarse-to-Fine Attention.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\D2X4QQ8X\\1609.html}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2025-11-08},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\9EAY7ZMR\\Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\RD4WI586\\2010.html}
}

@online{hoangDataset,
  title = {Hoang-Quoc-Trung/Fusion-Image-to-Latex-Datasets · {{Datasets}} at {{Hugging Face}}},
  date = {2024-04-17},
  url = {https://huggingface.co/datasets/hoang-quoc-trung/fusion-image-to-latex-datasets},
  urldate = {2025-11-08},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {C:\Users\jeppe\Zotero\storage\FMVRUN6F\fusion-image-to-latex-datasets.html}
}

@online{IntroductionTypstyleDocs,
  title = {Introduction - {{Typstyle Docs}}},
  url = {https://typstyle-rs.github.io/typstyle/},
  urldate = {2025-11-08},
  file = {C:\Users\jeppe\Zotero\storage\CIKR2XJW\typstyle.html}
}

@online{trocr,
  title = {{{TrOCR}}: {{Transformer-based Optical Character Recognition}} with {{Pre-trained Models}}},
  shorttitle = {{{TrOCR}}},
  author = {Li, Minghao and Lv, Tengchao and Chen, Jingye and Cui, Lei and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Li, Zhoujun and Wei, Furu},
  date = {2022-09-06},
  eprint = {2109.10282},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.10282},
  url = {http://arxiv.org/abs/2109.10282},
  urldate = {2025-11-08},
  abstract = {Text recognition is a long-standing research problem for document digitalization. Existing approaches are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments show that the TrOCR model outperforms the current state-of-the-art models on the printed, handwritten and scene text recognition tasks. The TrOCR models and code are publicly available at \textbackslash url\{https://aka.ms/trocr\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\K9JI7YZG\\Li et al. - 2022 - TrOCR Transformer-based Optical Character Recognition with Pre-trained Models.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\4HCXCAP5\\2109.html}
}

@online{MitexrsMitexLaTeX,
  title = {Mitex-Rs/Mitex: {{LaTeX}} Support for {{Typst}}, Powered by {{Rust}} and {{WASM}}. {{https://mitex-rs.github.io/mitex/}}},
  url = {https://github.com/mitex-rs/mitex},
  urldate = {2025-11-08},
  file = {C:\Users\jeppe\Zotero\storage\36Z7H7ZV\mitex.html}
}

@software{paran3xusParaN3xusTypress2025,
  title = {{{ParaN3xus}}/Typress},
  author = {ParaN3xus},
  date = {2025-10-28T10:45:49Z},
  origdate = {2024-07-17T11:43:29Z},
  url = {https://github.com/ParaN3xus/typress},
  urldate = {2025-11-08},
  abstract = {Typst Mathematical Expression OCR}
}

@online{tex2typ,
  title = {{{ParaN3xus}}/Tex2typ: {{LaTeX}} Math Equations to {{Typst}} Equations Conversion.},
  url = {https://github.com/ParaN3xus/tex2typ},
  urldate = {2025-11-08},
  file = {C:\Users\jeppe\Zotero\storage\HNA5HSDD\tex2typ.html}
}

@software{qwinsiQwinsiTex2typst2025,
  title = {Qwinsi/Tex2typst},
  author = {{qwinsi}},
  date = {2025-11-08T10:33:22Z},
  origdate = {2024-07-15T00:08:06Z},
  url = {https://github.com/qwinsi/tex2typst},
  urldate = {2025-11-08},
  abstract = {JavaScript library for conversion between TeX/LaTeX and Typst math code.},
  keywords = {latex,math-formula,tex,typst}
}

@software{Tex2typPythonCLI,
  title = {Tex2typ: {{A Python CLI}} Tool for Converting {{LaTeX}} Equations into {{Typst}} Equations},
  shorttitle = {Tex2typ},
  url = {https://Niels-Skovgaard-Jensen.github.io/tex2typ/},
  urldate = {2025-11-08},
  version = {0.0.3},
  keywords = {python,Software Development - Libraries - Python Modules},
  file = {C:\Users\jeppe\Zotero\storage\NJCB6ACI\tex2typ.html}
}

@online{TransformersTutorialsTrOCRMaster,
  title = {Transformers-{{Tutorials}}/{{TrOCR}} at Master · {{NielsRogge}}/{{Transformers-Tutorials}}},
  url = {https://github.com/NielsRogge/Transformers-Tutorials/tree/master/TrOCR},
  urldate = {2025-11-08},
  abstract = {This repository contains demos I made with the Transformers library by HuggingFace. - NielsRogge/Transformers-Tutorials},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\jeppe\Zotero\storage\78CSBKNS\TrOCR.html}
}

@online{typst,
  title = {Typst: {{The}} New Foundation for Documents},
  shorttitle = {Typst},
  date = {2025-11-07},
  url = {https://typst.app/},
  urldate = {2025-11-08},
  abstract = {Typst is the new foundation for documents. Sign up now and experience limitless power to write, create, and automate anything that you can fit on a page.},
  langid = {english},
  organization = {Typst}
}
@article{stanfordpaper,
  title = {Adaptation of {{OCR Models}} for {{LATEX Vision}}},
  author = {Chhadia, Nikash and Gupta, Sambhav},
  abstract = {This research focuses on OCR for LaTeX mathematical expressions, which is the problem of determining the LaTeX code used to generate a compiled expression in image form. We apply two approaches to this problem, namely (1) finetuning a general OCR model instead of training a model from scratch, and (2) utilizing a synthetically generated LaTeX dataset to enable training on a large set of data without manual data collection. Both of these approaches are applied with the intention of drastically reducing the quantity of resources and compute required to train an effective model for the task. We find that, after fine-tuning, although the model shows some success classifying simple expressions, performance rapidly breaks down for more complex expressions. We present potential explanations for this performance for a model which succeeds in OCR tasks, which include an oversized token set, undertraining of the model, and lack of CNNs for feature extraction. We also present an interesting visualization of the attention mechanism of the model, which shows bias towards the beginnings of expressions.},
  langid = {english},
  file = {C:\Users\jeppe\Zotero\storage\VZFGBI5E\Chhadia and Gupta - Adaptation of OCR Models for LATEX Vision.pdf}
}
@online{pix2text,
  title = {Pix2text-Mfr},
  author = {{Breezedeus}},
  date = {2025-11-29},
  url = {https://huggingface.co/breezedeus/pix2text-mfr},
  urldate = {2025-12-01},
  organization = {HuggingFace},
  file = {C:\Users\jeppe\Zotero\storage\JLEUIJID\pix2text-mfr.html}
}

@online{deit,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
  date = {2021-01-15},
  eprint = {2012.12877},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2012.12877},
  url = {http://arxiv.org/abs/2012.12877},
  urldate = {2025-12-03},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\UCN3SZYI\\Touvron et al. - 2021 - Training data-efficient image transformers & distillation through attention.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\PZ9L7XVC\\2012.html}
}

@online{transformer,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-02},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-12-03},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\EEN2FW76\\Vaswani et al. - 2023 - Attention Is All You Need.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\9YUGNSJR\\1706.html}
}

@online{minilm,
  title = {{{MiniLM}}: {{Deep Self-Attention Distillation}} for {{Task-Agnostic Compression}} of {{Pre-Trained Transformers}}},
  shorttitle = {{{MiniLM}}},
  author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  date = {2020-04-06},
  eprint = {2002.10957},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2002.10957},
  url = {http://arxiv.org/abs/2002.10957},
  urldate = {2025-12-03},
  abstract = {Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\CFIU48CM\\Wang et al. - 2020 - MiniLM Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\3VK3RWNM\\2002.html}
}

@online{crnn,
  title = {An {{End-to-End Trainable Neural Network}} for {{Image-based Sequence Recognition}} and {{Its Application}} to {{Scene Text Recognition}}},
  author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
  date = {2015-07-21},
  eprint = {1507.05717},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1507.05717},
  url = {http://arxiv.org/abs/1507.05717},
  urldate = {2025-12-04},
  abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\9NR9B8AK\\Shi et al. - 2015 - An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to S.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\3JS8KI56\\1507.html}
}

@inproceedings{sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    editor = "Blanco, Eduardo  and
      Lu, Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012/",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}."
}

@online{bpe,
  title = {Neural {{Machine Translation}} of {{Rare Words}} with {{Subword Units}}},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  date = {2016-06-10},
  eprint = {1508.07909},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1508.07909},
  url = {http://arxiv.org/abs/1508.07909},
  urldate = {2025-12-04},
  abstract = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\jeppe\\Zotero\\storage\\ZCCMVCT5\\Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subword Units.pdf;C\:\\Users\\jeppe\\Zotero\\storage\\HE34PHWL\\1508.html}
}
@online{jkDataset,
  title = {{{JeppeKlitgaard}}/Typst-Image-Dataset · {{Datasets}} at {{Hugging Face}}},
  author = {{Jeppe Klitgaard}},
  url = {https://huggingface.co/datasets/JeppeKlitgaard/typst-image-dataset},
  urldate = {2025-12-04},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  organization = {HuggingFace},
  file = {C:\Users\jeppe\Zotero\storage\B8Q4DS6E\typst-image-dataset.html}
}

@online{typstDocsMath,
  title = {Math – {{Typst Documentation}}},
  author = {{Typst Community}},
  url = {https://typst.app/docs/reference/math/},
  urldate = {2025-12-04},
  abstract = {Documentation for functions related to Math in Typst.},
  langid = {english},
  organization = {Typst Documentation},
  file = {C:\Users\jeppe\Zotero\storage\6L8FE4QQ\math.html}
}

@software{typstCodex,
  title = {Codex {{Project}}},
  author = {{Typst Community}},
  date = {2025-11-24T20:27:20Z},
  origdate = {2024-11-13T14:26:15Z},
  url = {https://github.com/typst/codex},
  urldate = {2025-12-04},
  abstract = {A human-friendly notation for Unicode symbols.},
  version = {0.2.0}
}
